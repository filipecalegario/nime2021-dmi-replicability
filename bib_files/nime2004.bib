@inproceedings{Fels2004,
  author = {Fels, Sidney S. and Kaastra, Linda and Takahashi, Sachiyo and Mccaig, Graeme},
  title = {Evolving Tooka: from Experiment to Instrument},
  pages = {1--6},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176595},
  url = {http://www.nime.org/proceedings/2004/nime2004_001.pdf},
  keywords = {Musician-centred design, two-person musical instrument.},
  abstract = {The Tooka was created as an exploration of two personinstruments. We have worked with two Tooka performers toenhance the original experimental device to make a musicalinstrument played and enjoyed by them. The main additions tothe device include: an additional button that behaves as amusic capture button, a bend sensor, an additional thumbactuated pressure sensor for vibrato, additional musicalmapping strategies, and new interfacing hardware. Thesedevelopments a rose through exper iences andrecommendations from the musicians playing it. In addition tothe changes to the Tooka, this paper describes the learningprocess and experiences of the musicians performing with theTooka.}
}

@inproceedings{Kapur2004,
  author = {Kapur, Ajay and Lazier, Ariel J. and Davidson, Philip L. and Wilson, Scott and Cook, Perry R.},
  title = {The Electronic Sitar Controller},
  pages = {7--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176623},
  url = {http://www.nime.org/proceedings/2004/nime2004_007.pdf},
  keywords = {atmel microcontroller,controller,electronic sitar,esitar,human computer interface,indian string controller,instrument graphical feedback,midi,veldt},
  abstract = {This paper describes the design of an Electronic Sitar controller, adigitally modified version of Saraswati's (the Hindu Goddess ofMusic) 19-stringed, pumpkin shelled, traditional North Indianinstrument. The ESitar uses sensor technology to extract gesturalinformation from a performer, deducing music information suchas pitch, pluck timing, thumb pressure, and 3-axes of head tilt totrigger real-time sounds and graphics. It allows for a variety oftraditional sitar technique as well as new performance methods.Graphical feedback allows for artistic display and pedagogicalfeedback. The ESitar uses a programmable Atmel microprocessorwhich outputs control messages via a standard MIDI jack.}
}

@inproceedings{Takahata2004,
  author = {Takahata, Masami and Shiraki, Kensuke and Sakane, Yutaka and Takebayashi, Yoichi},
  title = {Sound Feedback for Powerful Karate Training},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176673},
  url = {http://www.nime.org/proceedings/2004/nime2004_013.pdf},
  keywords = {Sound feedback, Karate, Learning environment, Wearable device},
  abstract = {We have developed new sound feedback for powerful karate training with pleasure, which enables to extract player's movement, understand player's activities, and generate them to sounds. We have designed a karate training environment which consists of a multimodal room with cameras, microphones, video displays and loud speakers, and wearable devices with a sensor and a sound generator. Experiments have been conducted on ten Karate beginnners for ten months to examine the effectiveness to learn appropriate body action and sharpness in basic punch called TSUKI. The experimental results suggest the proposed sound feedback and the training environments enable beginners to achieve enjoyable Karate.}
}

@inproceedings{Kaltenbrunner2004,
  author = {Kaltenbrunner, Martin and Geiger, G\''{u}nter and Jord\`{a}, Sergi},
  title = {Dynamic Patches for Live Musical Performance},
  pages = {19--22},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176621},
  url = {http://www.nime.org/proceedings/2004/nime2004_019.pdf},
  keywords = {dynamic patching,musical instrument,sound synthesis,tangible interfaces,visual programming},
  abstract = {This article reflects the current state of the reacTable* project,an electronic music instrument with a tangible table-basedinterface, which is currently under development at theAudiovisual Institute at the Universitat Pompeu Fabra. In thispaper we are focussing on the issue of Dynamic Patching,which is a particular and unique aspect of the sound synthesisand control paradigms of the reacTable*. Unlike commonvisual programming languages for sound synthesis, whichconceptually separate the patch building process from theactual musical performance, the reacTable* combines theconstruction and playing of the instrument in a unique way.The tangible interface allows direct manipulation control overany of the used building blocks, which physically representthe whole synthesizer function.}
}

@inproceedings{Young2004,
  author = {Young, Diana and Fujinaga, Ichiro},
  title = {AoBachi: A New Interface for {Japan}ese Drumming},
  pages = {23--26},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176687},
  url = {http://www.nime.org/proceedings/2004/nime2004_023.pdf},
  keywords = {bluetooth,drum stick,japanese drum,taiko,wireless},
  abstract = {We present a prototype of a new musical interface for Japanese drumming techniques and styles. Our design used in the Aobachi drumming sticks provides 5 gesture parameters (3 axes of acceleration, and 2 axes of angular velocity) for each of the two sticks and transmits this data wirelessly using BluetoothÂ® technology. This system utilizes minimal hardware embedded in the two drumming sticks, allowing for gesture tracking of drum strokes by an interface of traditional form, appearance, and feel. Aobachi is portable, versatile, and robust, and may be used for a variety of musical applications, as well as analytical studies.}
}

@inproceedings{Bryan-Kinns2004,
  author = {Bryan-Kinns, Nick and Healey, Patrick G.},
  title = {Daisyphone: Support for Remote Music Collaboration},
  pages = {27--30},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176583},
  url = {http://www.nime.org/proceedings/2004/nime2004_027.pdf},
  keywords = {collaboration,composition,improvisation,music},
  abstract = {We have seen many new and exciting developments in new interfaces for musical expression. In this paper we present the design of an interface for remote group music improvisation and composition - Daisyphone. The approach relies on players creating and editing short shared loops of music which are semi-synchronously updated. The interface emphasizes the looping nature of the music and is designed to be engaging and deployable on a wide range of interaction devices. Observations of the use of the tool with different levels of persistence of contribution are reported and discussed. Future developments centre around ways to string loops together into larger pieces (composition) and investigating suitable rates of decay to encourage more group improvisation.}
}

@inproceedings{Havel2004,
  author = {Havel, Christophe and Desainte-Catherine, Myriam},
  title = {Modeling an Air Percussion for Composition and Performance},
  pages = {31--34},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176609},
  url = {http://www.nime.org/proceedings/2004/nime2004_031.pdf},
  keywords = {Gesture analysis, virtual percussion, strike recognition.},
  abstract = {This paper presents a project involving a percussionist playing on a virtual percussion. Both artistic and technical aspects of the project are developed. Especially, a method forstrike recognition using the Flock of Birds is presented, aswell as its use for artistic purpose.}
}

@inproceedings{Nelson2004,
  author = {Nelson, Mark and Thom, Belinda},
  title = {A Survey of Real-Time {MIDI} Performance},
  pages = {35--38},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176643},
  url = {http://www.nime.org/proceedings/2004/nime2004_035.pdf},
  abstract = {Although MIDI is often used for computer-based interactive music applications, its real-time performance is rarely quantified, despite concerns about whether it is capable of adequate performance in realistic settings. We extend existing proposals for MIDI performance benchmarking so they are useful in realistic interactive scenarios, including those with heavy MIDI traffic and CPU load. We have produced a cross-platform freely-available testing suite that is easy to use, and have used it to survey the interactive performance of several commonly-used computer/MIDI setups. We describe the suite, summarize the results of our performance survey, and detail the benefits of this testing methodology.}
}

@inproceedings{Cont2004,
  author = {Cont, Arshia and Coduys, Thierry and Henry, Cyrille},
  title = {Real-time Gesture Mapping in Pd Environment using Neural Networks},
  pages = {39--42},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176589},
  url = {http://www.nime.org/proceedings/2004/nime2004_039.pdf},
  keywords = {Real-time gesture control, adaptive interfaces, Sensor and actuator technologies for musical applications, Musical mapping algorithms and intelligent controllers, Pure Data.},
  abstract = {In this paper, we describe an adaptive approach to gesture mapping for musical applications which serves as a mapping system for music instrument design. A neural network approach is chosen for this goal and all the required interfaces and abstractions are developed and demonstrated in the Pure Data environment. In this paper, we will focus on neural network representation and implementation in a real-time musical environment. This adaptive mapping is evaluated in different static and dynamic situations by a network of sensors sampled at a rate of 200Hz in real-time. Finally, some remarks are given on the network design and future works. }
}

@inproceedings{Talmudi2004,
  author = {Talmudi, Assaf K.},
  title = {The Decentralized Pianola: Evolving Mechanical Music Instruments using a Genetic Algorithm},
  pages = {43--46},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176675},
  url = {http://www.nime.org/proceedings/2004/nime2004_043.pdf},
  abstract = {This paper presents computer experiments concerning the decentralized pianola, a hypothetical mechanical music instrument, whose large-scale musical behavior is the result of local physical interactions between simple elements.Traditional mechanical music instruments like the pianola and the music box rely for their operation on the separation between a sequential memory unit and an execution unit. In a decentralized mechanical instrument, musical memory is an emergent global property of the system, undistinguishable from the execution process. Such a machine is botha score andan instrument. The paper starts by discussing the difference between sequential memory systems and systems exhibiting emergent decentralized musical behavior. Next, the use of particle system simulation for exploring virtual decentralized instruments is demonstrated, and the architecture for a simple decentralized instrument is outlined. The paper continues by describing the use of a genetic algorithm for evolving decentralized instruments that reproduce a given musical behavior.}
}

@inproceedings{Mandelis2004,
  author = {Mandelis, James and Husbands, Phil},
  title = {Don't Just Play it, Grow it! : Breeding Sound Synthesis and Performance Mappings},
  pages = {47--50},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176635},
  url = {http://www.nime.org/proceedings/2004/nime2004_047.pdf},
  keywords = {musical interaction,performance mapping,sound synthesis},
  abstract = {This paper describes the use of evolutionary and artificial life techniques in sound design and the development of performance mapping to facilitate the real-time manipulation of such sounds through some input device controlled by the performer. A concrete example of such a system is described which allows musicians without detailed knowledge and experience of sound synthesis techniques to interactively develop new sounds and performance manipulation mappings according to their own aesthetic judgements. Experiences with the system are discussed. }
}

@inproceedings{Shatin2004,
  author = {Shatin, Judith and Topper, David},
  title = {Tree Music: Composing with GAIA},
  pages = {51--54},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176663},
  url = {http://www.nime.org/proceedings/2004/nime2004_051.pdf},
  keywords = {Composition, new interfaces, interactive systems, open source, Real time audio, GUI controllers, video tracking},
  abstract = {In this report, we discuss Tree Music, an interactive computer music installation created using GAIA (Graphical Audio Interface Application), a new open-source interface for controlling the RTcmix synthesis and effects processing engine. Tree Music, commissioned by the University of Virginia Art Museum, used a wireless camera with a wide-angle lens to capture motion and occlusion data from exhibit visitors. We show how GAIA was used to structure and navigate the compositional space, and how this program supports both graphical and text-based programming in the same application. GAIA provides a GUI which combines two open-source applications: RTcmix and Perl.}
}

@inproceedings{DArcangelo2004,
  author = {D'Arcangelo, Gideon},
  title = {Recycling Music, Answering Back: Toward an Oral Tradition of Electronic Music},
  pages = {55--58},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176591},
  url = {http://www.nime.org/proceedings/2004/nime2004_055.pdf},
  keywords = {Call and response, turntablism, DJ tools, oral culture},
  abstract = {This essay outlines a framework for understanding newmusical compositions and performances that utilizepre-existing sound recordings. In attempting toarticulate why musicians are increasingly using soundrecordings in their creative work, the author calls fornew performance tools that enable the dynamic use ofpre-recorded music. }
}

@inproceedings{Jorda2004,
  author = {Jord\`{a}, Sergi},
  title = {Digital Instruments and Players: Part I -- Efficiency and Apprenticeship},
  pages = {59--63},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176619},
  url = {http://www.nime.org/proceedings/2004/nime2004_059.pdf},
  keywords = {Musical instruments design, learning curve, apprenticeship, musical efficiency.},
  abstract = {When envisaging new digital instruments, designers do not have to limit themselves to their sonic capabilities (which can be absolutely any), not even to their algorithmic power; they must be also especially careful about the instruments' conceptual capabilities, to the ways instruments impose or suggest to their players new ways of thinking, new ways of establishing relations, new ways of interacting, new ways of organizing time and textures; new ways, in short, of playing new musics. This article explores the dynamic relation that builds between the player and the instrument, introducing concepts such as efficiency, apprenticeship and learning curve It aims at constructing a framework in which the possibilities and the diversity of music instruments as well as the possibilities and the expressive freedom of human music performers could start being evaluated. }
}

@inproceedings{Pashenkov2004,
  author = {Pashenkov, Nikita},
  title = {A New Mix of Forgotten Technology: Sound Generation, Sequencing and Performance Using an Optical Turntable},
  pages = {64--67},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176651},
  url = {http://www.nime.org/proceedings/2004/nime2004_064.pdf},
  keywords = {Interaction, visualization, tangible interface, controllers, optical turntable, performance.},
  abstract = {This report presents a novel interface for musical performance which utilizes a record-player turntable augmented with a computation engine and a high-density optical sensing array. The turntable functions as a standalone step sequencer for MIDI events transmitted to a computer or another device and it is programmed in real-time using visual disks. The program instructions are represented on printed paper disks directly as characters of English alphabet that could be read by human as effectively as they are picked up by the machine's optical cartridge. The result is a tangible interface that allows the user to manipulate pre-arranged musical material by hand, by adding together instrumental tracks to form a dynamic mix. A functional implementation of this interface is discussed in view of historical background and other examples of electronic instruments for music creation and performance incorporating optical turntable as a central element.}
}

@inproceedings{Lee2004,
  author = {Lee, Eric and Nakra, Teresa M. and Borchers, Jan},
  title = {You're The Conductor: A Realistic Interactive Conducting System for Children},
  pages = {68--73},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176629},
  url = {http://www.nime.org/proceedings/2004/nime2004_068.pdf},
  keywords = {conducting systems,design patterns,gesture recogni-,interactive exhibits,real-time audio stretching,tion},
  abstract = {This paper describes the first system designed to allow children to conduct an audio and video recording of an orchestra. No prior music experience is required to control the orchestra, and the system uses an advanced algorithm to time stretch the audio in real-time at high quality and without altering the pitch. We will discuss the requirements and challenges of designing an interface to target our particular user group (children), followed by some system implementation details. An overview of the algorithm used for audio time stretching will also be presented. We are currently using this technology to study and compare professional and non-professional conducting behavior, and its implications when designing new interfaces for multimedia. You're the Conductor is currently a successful exhibit at the Children's Museum in Boston, USA.}
}

@inproceedings{OModhrain2004,
  author = {O'Modhrain, Sile and Essl, Georg},
  title = {PebbleBox and CrumbleBag: Tactile Interfaces for Granular Synthesis},
  pages = {74--79},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176647},
  url = {http://www.nime.org/proceedings/2004/nime2004_074.pdf},
  keywords = {Musical instrument, granular synthesis, haptic},
  abstract = {The PebbleBox and the CrumbleBag are examples of a granular interaction paradigm, in which the manipulation ofphysical grains of arbitrary material becomes the basis forinteracting with granular sound synthesis models. The soundsmade by the grains as they are manipulated are analysed,and parameters such as grain rate, grain amplitude andgrain density are extracted. These parameters are then usedto control the granulation of arbitrary sound samples in realtime. In this way, a direct link is made between the haptic sensation of interacting with grains and the control ofgranular sounds.}
}

@inproceedings{Paine2004,
  author = {Paine, Garth},
  title = {Gesture and Musical Interaction : Interactive Engagement Through Dynamic Morphology},
  pages = {80--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176649},
  url = {http://www.nime.org/proceedings/2004/nime2004_080.pdf},
  keywords = {dynamic,dynamic morphology,gesture,interaction,mapping,mind,music,orchestration,spectral morphology},
  abstract = {New Interfaces for Musical Expression must speak to the nature of 'instrument', that is, it must always be understood that the interface binds to a complex musical phenomenon. This paper explores the nature of engagement, the point of performance that occurs when a human being engages with a computer based instrument. It asks questions about the nature of the instrument in computer music and offers some conceptual models for the mapping of gesture to sonic outcomes.}
}

@inproceedings{Van-Nort2004,
  author = {Van Nort, Doug and Wanderley, Marcelo M. and Depalle, Philippe},
  title = {On the Choice of Mappings Based on Geometric Properties},
  pages = {87--91},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176681},
  url = {http://www.nime.org/proceedings/2004/nime2004_087.pdf},
  keywords = {Mapping, Interface Design, Interpolation, Computational Geometry},
  abstract = {The choice of mapping strategies to effectively map controller variables to sound synthesis algorithms is examined.Specifically, we look at continuous mappings that have ageometric representation. Drawing from underlying mathematical theory, this paper presents a way to compare mapping strategies, with the goal of achieving an appropriatematch between mapping and musical performance context.This method of comparison is applied to existing techniques,while a suggestion is offered on how to integrate and extendthis work through a new implementation.}
}

@inproceedings{Sheehan2004,
  author = {Sheehan, Brian},
  title = {The Squiggle: A Digital Musical Instrument},
  pages = {92--95},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176665},
  url = {http://www.nime.org/proceedings/2004/nime2004_092.pdf},
  keywords = {Digital musical instruments, real-time performance, scanned synthesis, pd, tactile interfaces, sensors, Shapetape, mapping.},
  abstract = {This paper discusses some of the issues pertaining to the design of digital musical instruments that are to effectively fill the role of traditional instruments (i.e. those based on physical sound production mechanisms). The design and implementation of a musical instrument that addresses some of these issues, using scanned synthesis coupled to a "smart" physical system, is described.}
}

@inproceedings{Gerhard2004,
  author = {Gerhard, David and Hepting, Daryl and Mckague, Matthew},
  title = {Exploration of the Correspondence between Visual and Acoustic Parameter Spaces},
  pages = {96--99},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176603},
  url = {http://www.nime.org/proceedings/2004/nime2004_096.pdf},
  keywords = {Multimedia creation and interaction, parameter space, visualization, sonification.},
  abstract = {This paper describes an approach to match visual and acoustic parameters to produce an animated musical expression.Music may be generated to correspond to animation, asdescribed here; imagery may be created to correspond tomusic; or both may be developed simultaneously. This approach is intended to provide new tools to facilitate bothcollaboration between visual artists and musicians and examination of perceptual issues between visual and acousticmedia. As a proof-of-concept, a complete example is developed with linear fractals as a basis for the animation, andarranged rhythmic loops for the music. Since both visualand acoustic elements in the example are generated fromconcise specifications, the potential of this approach to create new works through parameter space exploration is accentuated, however, there are opportunities for applicationto a wide variety of source material. These additional applications are also discussed, along with issues encounteredin development of the example.}
}

@inproceedings{Ramakrishnan2004,
  author = {Ramakrishnan, Chandrasekhar and Freeman, Jason and Varnik, Kristjan},
  title = {The Architecture of Auracle: a Real-Time, Distributed, Collaborative Instrument},
  pages = {100--103},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176657},
  url = {http://www.nime.org/proceedings/2004/nime2004_100.pdf},
  keywords = {Interactive Music Systems, Networking and Control, Voice and Speech Analysis, Auracle, JSyn, TransJam, Linear Prediction, Neural Networks, Voice Interface, Open Sound Control},
  abstract = {Auracle is a "group instrument," controlled by the voice, for real-time, interactive, distributed music making over the Internet. It is implemented in the Javaâ¢ programming language using a combination of publicly available libraries (JSyn and TransJam) and custom-built components. This paper describes how the various pieces --- the voice analysis, network communication, and sound synthesis --- are individually built and how they are combined to form Auracle.}
}

@inproceedings{Miyashita2004,
  author = {Miyashita, Homei and Nishimoto, Kazushi},
  title = {Thermoscore: A New-type Musical Score with Temperature Sensation},
  pages = {104--107},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176637},
  url = {http://www.nime.org/proceedings/2004/nime2004_104.pdf},
  keywords = {musical score, improvisation, peltier device, chroma profile},
  abstract = {In this paper, we propose Thermoscore, a musical score form-that dynamically alters the temperature of the instrument/player interface. We developed the first version of theThermoscore display by lining Peltier devices on piano keys.The system is controlled by MIDI notes-on messages from anMIDI sequencer, so that a composer can design songs that aresequences of temperature for each piano key. We also discussmethodologies for composing with this system, and suggesttwo approaches. The first is to make desirable keys (or otherkeys) hot. The second one uses chroma-profile, that is, a radarchart representation of the frequency of pitch notations in the-piece. By making keys of the same chroma hot in reverse proportion to the value of the chroma-profile, it is possible to-constrain the performer's improvisation and to bring the tonality space close to a certain piece.}
}

@inproceedings{Serafin2004,
  author = {Serafin, Stefania and Young, Diana},
  title = {Toward a Generalized Friction Controller: from the Bowed String to Unusual Musical Instruments},
  pages = {108--111},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176659},
  url = {http://www.nime.org/proceedings/2004/nime2004_108.pdf},
  abstract = {We present case studies of unusual instruments that share the same excitation mechanism as that of the bowed string. The musical saw, Tibetan singing bow, glass harmonica, and bowed cymbal all produce sound by rubbing a hard object on the surface of the instrument. For each, we discuss the design of its physical model and present a means for expressively controlling it. Finally, we propose a new kind of generalized friction controller to be used in all these examples.}
}

@inproceedings{Zaborowski2004,
  author = {Zaborowski, Philippe S.},
  title = {ThumbTec: A New Handheld Input Device},
  pages = {112--115},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176689},
  url = {http://www.nime.org/proceedings/2004/nime2004_112.pdf},
  keywords = {One-Thumb Input Device, HCI, Isometric Joystick, Mobile Computing, Handheld Devices, Musical Instrument.},
  abstract = {This paper describes ThumbTEC, a novel general purpose input device for the thumb or finger that is useful in a wide variety of applications from music to text entry. The device is made up of three switches in a row and one miniature joystick on top of the middle switch. The combination of joystick direction and switch(es) controls what note or alphanumeric character is selected by the finger. Several applications are detailed.}
}

@inproceedings{Torchia2004,
  author = {Torchia, Ryan H. and Lippe, Cort},
  title = {Techniques for Multi-Channel Real-Time Spatial Distribution Using Frequency-Domain Processing},
  pages = {116--119},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176679},
  url = {http://www.nime.org/proceedings/2004/nime2004_116.pdf},
  abstract = {The authors have developed several methods for spatially distributing spectral material in real-time using frequency-domain processing. Applying spectral spatialization techniques to more than two channels introduces a few obstacles, particularly with controllers, visualization and the manipulation of large amounts of control data. Various interfaces are presented which address these issues. We also discuss 3D âcubeâ controllers and visualizations, which go a long way in aiding usability. A range of implementations were realized, each with its own interface, automation, and output characteristics. We also explore a number of novel techniques. For example, a soundâs spectral components can be mapped in space based on its own componentsâ energy, or the energy of another signalâs components (a kind of spatial cross-synthesis). Finally, we address aesthetic concerns, such as perceptual and sonic coherency, which arise when sounds have been spectrally dissected and scattered across a multi-channel spatial field in 64, 128 or more spectral bands.}
}

@inproceedings{Hiraga2004,
  author = {Hiraga, Rumi and Bresin, Roberto and Hirata, Keiji and Katayose, Haruhiro},
  title = {Rencon 2004: Turing Test for Musical Expression},
  pages = {120--123},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176611},
  url = {http://www.nime.org/proceedings/2004/nime2004_120.pdf},
  keywords = {Rencon, Turing Test, Musical Expression, Performance Rendering},
  abstract = {Rencon is an annual international event that started in 2002. It has roles of (1) pursuing evaluation methods for systems whose output includes subjective issues, and (2) providing a forum for researches of several fields related to musical expression. In the past. Rencon was held as a workshop associated with a musical contest that provided a forum for presenting and discussing the latest research in automatic performance rendering. This year we introduce new evaluation methods of performance expression to Rencon: a Turing Test and a Gnirut Test, which is a reverse Turing Test, for performance expression. We have opened a section of the contests to any instruments and genre of music, including synthesized human voices.}
}

@inproceedings{Katayose2004,
  author = {Katayose, Haruhiro and Okudaira, Keita},
  title = {Using an Expressive Performance Template in a Music Conducting Interface},
  pages = {124--129},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176625},
  url = {http://www.nime.org/proceedings/2004/nime2004_124.pdf},
  keywords = {Rencon, interfaces for musical expression, visualization},
  abstract = {This paper describes an approach for playing expressivemusic, as it refers to a pianist's expressiveness, with atapping-style interface. MIDI-formatted expressiveperformances played by pianists were first analyzed andtransformed into performance templates, in which thedeviations from a canonical description was separatelydescribed for each event. Using one of the templates as askill complement, a player can play music expressivelyover and under the beat level. This paper presents ascheduler that allows a player to mix her/his own intensionand the expressiveness in the performance template. Theresults of a forty-subject user study suggest that using theexpression template contributes the subject's joy of playingmusic with the tapping-style performance interface. Thisresult is also supported by a brain activation study that wasdone using a near-infrared spectroscopy (NIRS).Categories and Subject DescriptorsH.5.5 [Information Interfaces and Presentation]: Sound andMusic Computing methodologies and techniques.}
}

@inproceedings{Kawahara2004,
  author = {Kawahara, Hideki and Banno, Hideki and Morise, Masanori},
  title = {Acappella Synthesis Demonstrations using RWC Music Database},
  pages = {130--131},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176627},
  url = {http://www.nime.org/proceedings/2004/nime2004_130.pdf},
  keywords = {Rencon, Acappella, RWCdatabase, STRAIGHT, morphing},
  abstract = {A series of demonstrations of synthesized acappella songsbased on an auditory morphing using STRAIGHT [5] willbe presented. Singing voice data for morphing were extracted from the RWCmusic database of musical instrument sound. Discussions on a new extension of the morphing procedure to deal with vibrato will be introduced basedon the statistical analysis of the database and its effect onsynthesized acappella will also be demonstrated.}
}

@inproceedings{Dannenberg2004,
  author = {Dannenberg, Roger B.},
  title = {Aura II: Making Real-Time Systems Safe for Music},
  pages = {132--137},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176593},
  url = {http://www.nime.org/proceedings/2004/nime2004_132.pdf},
  abstract = {Real-time interactive software can be difficult to construct and debug. Aura is a software platform to facilitate highly interactive systems that combine audio signal processing, sophisticated control, sensors, computer animation, video processing, and graphical user interfaces. Moreover, Aura is open-ended, allowing diverse software components to be interconnected in a real-time framework. A recent assessment of Aura has motivated a redesign of the communication system to support remote procedure call. In addition, the audio signal processing framework has been altered to reduce programming errors. The motivation behind these changes is discussed, and measurements of run-time performance offer some general insights for system designers.}
}

@inproceedings{Wang2004,
  author = {Wang, Ge and Cook, Perry R.},
  title = {On-the-fly Programming: Using Code as an Expressive Musical Instrument},
  pages = {138--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176683},
  url = {http://www.nime.org/proceedings/2004/nime2004_138.pdf},
  keywords = {code as interface,compiler,concurrency,concurrent audio programming,on-the-fly programming,real-,synchronization,synthesis,time,timing,virtual machine},
  abstract = {On-the-fly programming is a style of programming in which the programmer/performer/composer augments and modifies the program while it is running, without stopping or restarting, in order to assert expressive, programmable control at runtime. Because of the fundamental powers of programming languages, we believe the technical and aesthetic aspects of on-the-fly programming are worth exploring. In this paper, we present a formalized framework for on-the-fly programming, based on the ChucK synthesis language, which supports a truly concurrent audio programming model with sample-synchronous timing, and a highly on-the-fly style of programming. We first provide a well-defined notion of on-thefly programming. We then address four fundamental issues that confront the on-the-fly programmer: timing, modularity, conciseness, and flexibility. Using the features and properties of ChucK, we show how it solves many of these issues. In this new model, we show that (1) concurrency provides natural modularity for on-the-fly programming, (2) the timing mechanism in ChucK guarantees on-the-fly precision and consistency, (3) the Chuck syntax improves conciseness, and (4) the overall system is a useful framework for exploring on-the-fly programming. Finally, we discuss the aesthetics of on-the-fly performance. }
}

@inproceedings{Lew2004,
  author = {Lew, Michael},
  title = {Live Cinema: Designing an Instrument for Cinema Editing as a Live Performance},
  pages = {144--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176631},
  url = {http://www.nime.org/proceedings/2004/nime2004_144.pdf},
  keywords = {live cinema, video controller, visual music, DJ, VJ, film editing, tactile interface, two-hand interaction, improvisation, performance, narrative structure.},
  abstract = {This paper describes the design of an expressive tangible interface for cinema editing as a live performance. A short survey of live video practices is provided. The Live Cinema instrument is a cross between a musical instrument and a film editing tool, tailored for improvisational control as well as performance presence. Design specifications for the instrument evolved based on several types of observations including: our own performances in which we used a prototype based on available tools; an analysis of performative aspects of contemporary DJ equipment; and an evaluation of organizational aspects of several generations of film editing tools. Our instrument presents the performer with a large canvas where projected images can be grabbed and moved around with both hands simultaneously; the performer also has access to two video drums featuring haptic display to manipulate the shots and cut between streams. The paper ends with a discussion of issues related to the tensions between narrative structure and hands-on control, live and recorded arts and the scoring of improvised films. }
}

@inproceedings{Poepel2004,
  author = {Poepel, Cornelius},
  title = {Synthesized Strings for String Players},
  pages = {150--153},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176655},
  url = {http://www.nime.org/proceedings/2004/nime2004_150.pdf},
  keywords = {Electronic bowed string instrument, playability, musical instrument design, human computer interface, oscillation controlled sound synthesis},
  abstract = {A system is introduced that allows a string player to control a synthesis engine with the gestural skills he is used to. The implemented system is based on an electric viola and a synthesis engine that is directly controlled by the unanalysed audio signal of the instrument and indirectly by control parameters mapped to the synthesis engine. This method offers a highly string-specific playability, as it is sensitive to the kinds of musical articulation produced by traditional playing techniques. Nuances of sound variation applied by the player will be present in the output signal even if those nuances are beyond traditionally measurable parameters like pitch, amplitude or brightness. The relatively minimal hardware requirements make the instrument accessible with little expenditure.}
}

@inproceedings{Tanaka2004,
  author = {Tanaka, Atau},
  title = {Mobile Music Making},
  pages = {154--156},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176677},
  url = {http://www.nime.org/proceedings/2004/nime2004_154.pdf},
  keywords = {mobile music,peer-to-peer,wireless ad-hoc networks},
  abstract = {We present a system for collaborative musical creation onmobile wireless networks. The work extends on simple peerto-peer file sharing systems towards ad-hoc mobility andstreaming. It extends upon music listening from a passiveact to a proactive, participative activity. The system consistsof a network based interactive music engine and a portablerendering player. It serves as a platform for experiments onstudying the sense of agency in collaborative creativeprocess, and requirements for fostering musical satisfactionin remote collaboration. }
}

@inproceedings{Flety2004,
  author = {Fl\'{e}ty, Emmanuel and Leroy, Nicolas and Ravarini, Jean-Christophe and Bevilacqua, Fr\'{e}d\'{e}ric},
  title = {Versatile Sensor Acquisition System Utilizing Network Technology},
  pages = {157--160},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176597},
  url = {http://www.nime.org/proceedings/2004/nime2004_157.pdf},
  keywords = {Gesture, Sensors, Ethernet, 802.11, Computer Music.},
  abstract = {This paper reports our recent developments on sensor acquisition systems, taking advantage of computer network technology. We present a versatile hardware system which can be connected to wireless modules, Analog to Digital Converters, and enables Ethernet communication. We are planning to make freely available the design of this architecture. We describe also several approaches we tested for wireless communication. Such technology developments are currently used in our newly formed Performance Arts Technology Group.}
}

@inproceedings{Gaye2004,
  author = {Gaye, Lalya and Holmquist, Lars E.},
  title = {In Duet with Everyday Urban Settings: A User Study of Sonic City},
  pages = {161--164},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176601},
  url = {http://www.nime.org/proceedings/2004/nime2004_161.pdf},
  keywords = {User study, new interface for musical expression, interactive music, wearable computing, mobility, context-awareness.},
  abstract = {Sonic City is a wearable system enabling the use of the urban environment as an interface for real-time electronic music making, when walking through and interacting with a city. The device senses everyday interactions and surrounding contexts, and maps this information in real time to the sound processing of urban sounds. We conducted a short-term study with various participants using our prototype in everyday settings. This paper describes the course of the study and preliminary results in terms of how the participants used and experienced the system. These results showed that the city was perceived as the main performer but that the user improvised different tactics and ad hoc interventions to actively influence and participate in how the music was created. }
}

@inproceedings{Franco2004,
  author = {Franco, Enrique and Griffith, Niall J. and Fernstr\''{o}m, Mikael},
  title = {Issues for Designing a Flexible Expressive Audiovisual System for Real-time Performance \& Composition},
  pages = {165--168},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176599},
  url = {http://www.nime.org/proceedings/2004/nime2004_165.pdf},
  keywords = {Audiovisual, composition, performance, gesture, image, representation, mapping, expressiveness.},
  abstract = {This paper begins by evaluating various systems in terms of factors for building interactive audiovisual environments. The main issues for flexibility and expressiveness in the generation of dynamic sounds and images are then isolated. The design and development of an audiovisual system prototype is described at the end. }
}

@inproceedings{Silva2004,
  author = {de Silva, Gamhewage C. and Smyth, Tamara and Lyons, Michael J.},
  title = {A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models},
  pages = {169--172},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176667},
  url = {http://www.nime.org/proceedings/2004/nime2004_169.pdf},
  keywords = {Mouth Controller, Face Tracking, Bioacoustics},
  abstract = {We describe a simple, computationally light, real-time system for tracking the lower face and extracting informationabout the shape of the open mouth from a video sequence.The system allows unencumbered control of audio synthesismodules by action of the mouth. We report work in progressto use the mouth controller to interact with a physical modelof sound production by the avian syrinx.}
}

@inproceedings{Nagashima2004,
  author = {Nagashima, Yoichi},
  title = {Measurement of Latency in Interactive Multimedia Art},
  pages = {173--176},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176641},
  url = {http://www.nime.org/proceedings/2004/nime2004_173.pdf},
  abstract = {In this paper, I would like to introduce my experimental study of multimedia psychology. My initial focus of investigation is the interaction between perceptions of auditory and visual beats. When the musical and graphical beats are completely synchronized with each other, as in a music video for promotional purposes, the audience feels that they are natural and comforting. My initial experiment has proved that the actual tempos of music and images are a little different. If a slight timelag exists between the musical and pictorial beats, the audience tries to keep them in synchronization by unconsciously changing the interpretation of the time-based beat points. As the lag increases over time, the audience seems to perceive that the beat synchronization has changed from being more downbeat to more upbeat, and continues enjoying it. I have developed an experiment system that can generateand control out-of-phase visual and auditory beats in real time, and have tested many subjects with it. This paper describes the measurement of time lags generated in the experiment system, as part of my psychological experiment.}
}

@inproceedings{Ishida2004,
  author = {Ishida, Katsuhisa and Kitahara, Tetsuro and Takeda, Masayuki},
  title = {ism: Improvisation Supporting System based on Melody Correction},
  pages = {177--180},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176617},
  url = {http://www.nime.org/proceedings/2004/nime2004_177.pdf},
  keywords = {Improvisation support, jam session, melody correction, N-gram model, melody modeling, musical instrument},
  abstract = {In this paper, we describe a novel improvisation supporting system based on correcting musically unnatural melodies. Since improvisation is the musical performance style that involves creating melodies while playing, it is not easy even for the people who can play musical instruments. However, previous studies have not dealt with improvisation support for the people who can play musical instruments but cannot improvise. In this study, to support such players' improvisation, we propose a novel improvisation supporting system called ism, which corrects musically unnatural melodies automatically. The main issue in realizing this system is how to detect notes to be corrected (i.e., musically unnatural or inappropriate). We propose a method for detecting notes to be corrected based on the N-gram model. This method first calculates N-gram probabilities of played notes, and then judges notes with low N-gram probabilities to be corrected. Experimental results show that the N-gram-based melody correction and the proposed system are useful for supporting improvisation.}
}

@inproceedings{Singer2004,
  author = {Singer, Eric and Feddersen, Jeff and Redmon, Chad and Bowen, Bil},
  title = {LEMUR's Musical Robots},
  pages = {181--184},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176669},
  url = {http://www.nime.org/proceedings/2004/nime2004_181.pdf},
  keywords = {additional computer or special,commands allows,familiar tools with no,improvisations,the musician or composer,to control the instrument,use of standard midi,using},
  abstract = {This paper describes new work and creations of LEMUR, agroup of artists and technologists creating robotic musicalinstruments.}
}

@inproceedings{Hornof2004,
  author = {Hornof, Anthony J. and Sato, Linda},
  title = {EyeMusic: Making Music with the Eyes},
  pages = {185--188},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176613},
  url = {http://www.nime.org/proceedings/2004/nime2004_185.pdf},
  keywords = {Electronic music composition, eye movements, eye tracking, human-computer interaction, Max/MSP.},
  abstract = {Though musical performers routinely use eye movements to communicate with each other during musical performances, very few performers or composers have used eye tracking devices to direct musical compositions and performances. EyeMusic is a system that uses eye movements as an input to electronic music compositions. The eye movements can directly control the music, or the music can respond to the eyes moving around a visual scene. EyeMusic is implemented so that any composer using established composition software can incorporate prerecorded eye movement data into their musical compositions.}
}

@inproceedings{Argo2004,
  author = {Argo, Mark},
  title = {The Slidepipe: A Timeline-Based Controller for Real-Time Sample Manipulation},
  pages = {189--192},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176581},
  url = {http://www.nime.org/proceedings/2004/nime2004_189.pdf},
  keywords = {Controller, Sample Manipulation, Live Performance, Open Sound Control, Human Computer Interaction},
  abstract = {When working with sample-based media, a performer is managing timelines, loop points, sample parameters and effects parameters. The Slidepipe is a performance controller that gives the artist a visually simple way to work with their material. Its design is modular and lightweight, so it can be easily transported and quickly assembled. Also, its large stature magnifies the gestures associated with its play, providing a more convincing performance. In this paper, I will describe what the controller is, how this new controller interface has affected my live performance, and how it can be used in different performance scenarios. }
}

@inproceedings{Burtner2004,
  author = {Burtner, Matthew},
  title = {A Theory of Modulated Objects for New Shamanic Controller Design},
  pages = {193--196},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176585},
  url = {http://www.nime.org/proceedings/2004/nime2004_193.pdf},
  keywords = {Music and Video Controllers, New Interface Design, Music Composition, Multimedia, Mythology, Shamanism, Ecoacoustics},
  abstract = {This paper describes a theory for modulated objects based on observations of recent musical interface design trends. The theory implies extensions to an object-based approach to controller design. Combining NIME research with ethnographic study of shamanic traditions. The author discusses the creation of new controllers based on the shamanic use of ritual objects.}
}

@inproceedings{Pelletier2004,
  author = {Pelletier, Jean-Marc},
  title = {A Shape-Based Approach to Computer Vision Musical Performance Systems},
  pages = {197--198},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176653},
  url = {http://www.nime.org/proceedings/2004/nime2004_197.pdf},
  keywords = {computer vision,image analysis,maxmsp,morphology,musical},
  abstract = {In this paper, I will describe a computer vision-based musical performance system that uses morphological assessments to provide control data. Using shape analysis allows the system to provide qualitative descriptors of the scene being captured while ensuring its use in a wide variety of different settings. This system was implemented under Max/MSP/Jitter, augmented with a number of external objects. (1)}
}

@inproceedings{Hughes2004,
  author = {Hughes, Stephen and Cannon, Cormac and O'Modhrain, Sile},
  title = {Epipe : A Novel Electronic Woodwind Controller},
  pages = {199--200},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176615},
  url = {http://www.nime.org/proceedings/2004/nime2004_199.pdf},
  keywords = {woodwind controller, variable tonehole control, MIDI, capacitive sensing},
  abstract = {The Epipe is a novel electronic woodwind controller with continuous tonehole coverage sensing, an initial design for which was introduced at NIME '03. Since then, we have successfully completed two fully operational prototypes. This short paper describes some of the issues encountered during the design and construction of this controller. It also details our own early experiences and impressions of the interface as well as its technical specifications. }
}

@inproceedings{Morris2004,
  author = {Morris, Geoffrey C. and Leitman, Sasha and Kassianidou, Marina},
  title = {SillyTone Squish Factory},
  pages = {201--202},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176639},
  url = {http://www.nime.org/proceedings/2004/nime2004_201.pdf},
  abstract = {This paper describes the SillyTone Squish Factory, a haptically engaging musical interface. It contains the motivation behind the device's development, a description of the interface, various mappings of the interface to musical applications, details of its construction, and the requirements to demo the interface. }
}

@inproceedings{Steiner2004,
  author = {Steiner, Hans-Christoph},
  title = {StickMusic: Using Haptic Feedback with a Phase Vocoder},
  pages = {203--204},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176671},
  url = {http://www.nime.org/proceedings/2004/nime2004_203.pdf},
  keywords = {haptic feedback, gestural control, performance, joystick, mouse},
  abstract = {StickMusic is an instrument comprised of two haptic devices, a joystick and a mouse, which control a phase vocoder in real time. The purpose is to experiment with ideas of how to apply haptic feedback when controlling synthesis algorithms that have no direct analogy to methods of generating sound in the physical world. }
}

@inproceedings{Coduys2004,
  author = {Coduys, Thierry and Henry, Cyrille and Cont, Arshia},
  title = {TOASTER and KROONDE: High-Resolution and High- Speed Real-time Sensor Interfaces},
  pages = {205--206},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176587},
  url = {http://www.nime.org/proceedings/2004/nime2004_205.pdf},
  keywords = {Interface, Sensors, Calibration, Precision, OSC, Pure Data, Max/MSP.},
  abstract = {High capacity of transmission lines (Ethernet in particular) is much higher than what imposed by MIDI today. So it is possible to use capturing interfaces with high-speed and high-resolution, thanks to the OSC protocol, for musical synthesis (either in realtime or non real-time). These new interfaces offer many advantages, not only in the area of musical composition with use of sensors but also in live and interactive performances. In this manner, the processes of calibration and signal processing are delocalized on a personal computer and augments possibilities of processing. In this demo, we present two hardware interfaces developed in La kitchen with corresponding processing to achieve a high-resolution, high-speed sensor processing for musical applications. }
}

@inproceedings{Goto2004,
  author = {Goto, Suguru and Suzuki, Takahiko},
  title = {The Case Study of Application of Advanced Gesture Interface and Mapping Interface, Virtual Musical Instrument "Le SuperPolm" and Gesture Controller "BodySuit"},
  pages = {207--208},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176605},
  url = {http://www.nime.org/proceedings/2004/nime2004_207.pdf},
  keywords = {Virtual Musical Instrument, Gesture Controller, Mapping Interface},
  abstract = {We will discuss the case study of application of the Virtual Musical Instrument and Sound Synthesis. Doing this application, the main subject is advanced Mapping Interface in order to connect these. For this experiment, our discussion also refers to Neural Network, as well as a brief introduction of the Virtual Musical Instrument "Le SuperPolm" and Gesture Controller "BodySuit".}
}

@inproceedings{Won2004,
  author = {Won, Sook Y. and Chan, Humane and Liu, Jeremy},
  title = {Light Pipes: A Light Controlled {MIDI} Instrument},
  pages = {209--210},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176685},
  url = {http://www.nime.org/proceedings/2004/nime2004_209.pdf},
  keywords = {Controllers, MIDI, light sensors, Pure Data.},
  abstract = {In this paper, we describe a new MIDI controller, the Light Pipes. The Light Pipes are a series of pipes that respond to incident light. The paper will discuss the design of the instrument, and the prototype we built. A piece was composed for the instrument using algorithms designed in Pure Data.}
}

@inproceedings{Lippit2004,
  author = {Lippit, Takuro M.},
  title = {Realtime Sampling System for the Turntablist, Version 2: 16padjoystickcontroller},
  pages = {211--212},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176633},
  url = {http://www.nime.org/proceedings/2004/nime2004_211.pdf},
  keywords = {DJ, Turntablism, Realtime Sampling, MAX/MSP, Microchip PIC microcontroller, MIDI},
  abstract = {In this paper, I describe a realtime sampling system for theturntablist, and the hardware and software design of the secondprototype, 16padjoystickcontroller.}
}

@inproceedings{Sharon2004,
  author = {Sharon, Michael E.},
  title = {The Stranglophone: Enhancing Expressiveness In Live Electronic Music},
  pages = {213--214},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176661},
  url = {http://www.nime.org/proceedings/2004/nime2004_213.pdf},
  keywords = {gestural control, mapping, Pure Data (pd), accelerometers, MIDI, microcontrollers, synthesis, musical instruments},
  abstract = {This paper describes the design and on-going development of an expressive gestural MIDI interface and how this couldenhance live performance of electronic music.}
}

@inproceedings{Hashida2004,
  author = {Hashida, Tomoko and Kakehi, Yasuaki and Naemura, Takeshi},
  title = {Ensemble System with i-trace},
  pages = {215--216},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2004},
  address = {Hamamatsu, Japan},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1176607},
  url = {http://www.nime.org/proceedings/2004/nime2004_215.pdf},
  keywords = {Improvisational Ensemble Play, Contrapuntal Music, Human Tracking, Traces, Spatially Augmented Reality},
  abstract = {This paper proposes an interface for improvisational ensemble plays which synthesizes musical sounds and graphical images on the floor from people's act of "walking". The aim of this paper is to develop such a system that enables nonprofessional people in our public spaces to play good contrapuntal music without any knowledge of music theory. The people are just walking. This system is based on the i-trace system [1] which can capture the people's behavior and give some visual feedback. }
}

