@inproceedings{KimBoyle2008,
  author = {Kim-Boyle, David},
  title = {Network Musics --- Play , Engagement and the Democratization of Performance},
  pages = {3--8},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179579},
  url = {http://www.nime.org/proceedings/2008/nime2008_003.pdf},
  keywords = {Networks, collaborative, open-form, play, interface. },
  abstract = {The rapid development of network communicationtechnologies has allowed composers to create new ways inwhich to directly engage participants in the exploration of newmusical environments. A number of distinctive aestheticapproaches to the musical application of networks will beoutlined in this paper each of which is mediated andconditioned by the technical and aesthetic foundations of thenetwork technologies themselves. Recent work in the field byartists such as Atau Tanaka and Metraform will be examined, aswill some of the earlier pioneering work in the genre by MaxNeuhaus. While recognizing the historical context ofcollaborative work, the ,
,
author will examine how the strategiesemployed in the work of these artists have helped redefine anew aesthetics of engagement in which play, spatial andtemporal dislocation are amongst the genre's definingcharacteristics.}
}

@inproceedings{Barbosa2008,
  author = {Barbosa, \`{A}lvaro},
  title = {Ten-Hand Piano : A Networked Music Installation},
  pages = {9--12},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179487},
  url = {http://www.nime.org/proceedings/2008/nime2008_009.pdf},
  keywords = {algorithmic composition,behavioral driven,electronic music instruments,interfaces,network music instruments,nime08,performance,public music,real-time collaborative,sound},
  abstract = {This paper presents the latest developments of the Public Sound Objects (PSOs) system, an experimental framework to implement and test new concepts for Networked Music. The project of a Public interactive installation using the PSOs system was commissioned in 2007 by Casa da Musica, the main concert hall space in Porto. It resulted in a distributed musical structure with up to ten interactive performance terminals distributed along the Casa da Musica's hallways, collectively controlling a shared acoustic piano. The installation allows the visitors to collaborate remotely with each other, within the building, using a software interface custom developed to facilitate collaborative music practices and with no requirements in terms previous knowledge of musical performance. }
}

@inproceedings{Wozniewski2008,
  author = {Wozniewski, Mike and Bouillot, Nicolas and Settel, Zack and Cooperstock, Jeremy R.},
  title = {Large-Scale Mobile Audio Environments for Collaborative Musical Interaction},
  pages = {13--18},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179651},
  url = {http://www.nime.org/proceedings/2008/nime2008_013.pdf},
  keywords = {sonic navigation, mobile music, spatial interaction, wireless audio streaming, locative media, collaborative interfaces },
  abstract = {New application spaces and artistic forms can emerge whenusers are freed from constraints. In the general case ofhuman-computer interfaces, users are often confined to afixed location, severely limiting mobility. To overcome thisconstraint in the context of musical interaction, we presenta system to manage large-scale collaborative mobile audioenvironments, driven by user movement. Multiple participants navigate through physical space while sharing overlaid virtual elements. Each user is equipped with a mobilecomputing device, GPS receiver, orientation sensor, microphone, headphones, or various combinations of these technologies. We investigate methods of location tracking, wireless audio streaming, and state management between mobiledevices and centralized servers. The result is a system thatallows mobile users, with subjective 3-D audio rendering,to share virtual scenes. The audio elements of these scenescan be organized into large-scale spatial audio interfaces,thus allowing for immersive mobile performance, locativeaudio installations, and many new forms of collaborativesonic activity.}
}

@inproceedings{Fraietta2008,
  author = {Fraietta, Angelo},
  title = {Open Sound Control : Constraints and Limitations},
  pages = {19--23},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179537},
  url = {http://www.nime.org/proceedings/2008/nime2008_019.pdf},
  keywords = {a,data transmission protocols,gestural controllers,has been implemented as,midi,nime08,open sound control,osc},
  abstract = {Open Sound Control (OSC) is being used successfully as amessaging protocol among many computers, gesturalcontrollers and multimedia systems. Although OSC hasaddressed some of the shortcomings of MIDI, OSC cannotdeliver on its promises as a real-time communication protocolfor constrained embedded systems. This paper will examinesome of the advantages but also dispel some of the mythsconcerning OSC. The paper will also describe how some of thebest features of OSC can be used to develop a lightweightprotocol that is microcontroller friendly.}
}

@inproceedings{Bozzolan2008,
  author = {Bozzolan, Matteo and Cospito, Giovanni},
  title = {SMuSIM : a Prototype of Multichannel Spatialization System with Multimodal Interaction Interface},
  pages = {24--27},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179501},
  url = {http://www.nime.org/proceedings/2008/nime2008_024.pdf},
  keywords = {Sound spatialization, multimodal interaction, interaction interfaces, EyesWeb, Pure data. },
  abstract = {The continuous evolutions in the human-computer interfaces field have allowed the development of control devicesthat let have a more and more intuitive, gestural and noninvasive interaction.Such devices find a natural employment also in the musicapplied informatics and in particular in the electronic music,always searching for new expressive means.This paper presents a prototype of a system for the realtime control of sound spatialization in a multichannel configuration with a multimodal interaction interface. The spatializer, called SMuSIM, employs interaction devices thatrange from the simple and well-established mouse and keyboard to a classical gaming used joystick (gamepad), finallyexploiting more advanced and innovative typologies basedon image analysis (as a webcam).}
}

@inproceedings{Nash2008,
  author = {Nash, Chris and Blackwell, Alan},
  title = {Realtime Representation and Gestural Control of Musical Polytempi},
  pages = {28--33},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179603},
  url = {http://www.nime.org/proceedings/2008/nime2008_028.pdf},
  keywords = {composition,gesture,nime08,performance,polytempi,realtime,tempo},
  abstract = {Over the last century, composers have made increasingly ambitious experiments with musical time, but have been impeded in expressing more temporally-complex musical processes by the limitations of both music notations and human performers. In this paper, we describe a computer-based notation and gestural control system for independently manipulating the tempi of musical parts within a piece, at performance time. We describe how the problem was approached, drawing upon feedback and suggestions from consultations across multiple disciplines, seeking analogous problems in other fields. Throughout, our approach is guided and, ultimately, assessed by an established professional composer, who was able to interact with a working prototype of the system. }
}

@inproceedings{Laurson2008,
  author = {Laurson, Mikael and Kuuskankare, Mika},
  title = {Towards Idiomatic and Flexible Score-based Gestural Control with a Scripting Language},
  pages = {34--37},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179589},
  url = {http://www.nime.org/proceedings/2008/nime2008_034.pdf},
  keywords = {synthesis control, expressive timing, playing styles }
}

@inproceedings{Bouenard2008,
  author = {Bou\''{e}nard, Alexandre and Gibet, Sylvie and Wanderley, Marcelo M.},
  title = {Enhancing the Visualization of Percussion Gestures by Virtual Character Animation},
  pages = {38--43},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179497},
  url = {http://www.nime.org/proceedings/2008/nime2008_038.pdf},
  keywords = {Gesture and sound, interface, percussion gesture, virtual character, interaction. },
  abstract = {A new interface for visualizing and analyzing percussion gestures is presented, proposing enhancements of existing motion capture analysis tools. This is achieved by offering apercussion gesture analysis protocol using motion capture.A virtual character dynamic model is then designed in order to take advantage of gesture characteristics, yielding toimprove gesture analysis with visualization and interactioncues of different types.}
}

@inproceedings{Young2008,
  author = {Young, Diana},
  title = {Classification of Common Violin Bowing Techniques Using Gesture Data from a Playable Measurement System},
  pages = {44--48},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177457},
  url = {http://www.nime.org/proceedings/2008/nime2008_044.pdf},
  keywords = {bowing, gesture, playing technique, principal component anal- ysis, classification }
}

@inproceedings{Pakarinen2008,
  author = {Pakarinen, Jyri and V\''{a}lim\''{a}ki, Vesa and Puputti, Tapio},
  title = {Slide Guitar Synthesizer with Gestural Control},
  pages = {49--52},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179607},
  url = {http://www.nime.org/proceedings/2008/nime2008_049.pdf},
  keywords = {Sound synthesis, slide guitar, gesture control, physical mod- eling },
  abstract = {This article discusses a virtual slide guitar instrument, recently introduced in [7]. The instrument consists of a novelphysics-based synthesis model and a gestural user interface.The synthesis engine uses energy-compensated time-varyingdigital waveguides. The string algorithm also contains aparametric model for synthesizing the tube-string contactsounds. The real-time virtual slide guitar user interface employs optical gesture recognition, so that the user can playthis virtual instrument simply by making slide guitar playing gestures in front of a camera.}
}

@inproceedings{Lahdeoja2008,
  author = {L\''{a}hdeoja, Otso},
  title = {An Approach to Instrument Augmentation : the Electric Guitar},
  pages = {53--56},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179585},
  url = {http://www.nime.org/proceedings/2008/nime2008_053.pdf},
  keywords = {Augmented instrument, electric guitar, gesture-sound relationship }
}

@inproceedings{Raisanen2008,
  author = {R\''{a}is\''{a}nen, Juhani},
  title = {Sormina -- a New Virtual and Tangible Instrument},
  pages = {57--60},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179617},
  url = {http://www.nime.org/proceedings/2008/nime2008_057.pdf},
  keywords = {Gestural controller, digital musical instrument, usability, music history, design. },
  abstract = {This paper describes the Sormina, a new virtual and tangibleinstrument, which has its origins in both virtual technology andthe heritage of traditional instrument design. The motivationbehind the project is presented, as well as hardware andsoftware design. Insights gained through collaboration withacoustic musicians are presented, as well as comparison tohistorical instrument design.}
}

@inproceedings{Berdahl2008a,
  author = {Berdahl, Edgar and Steiner, Hans-Christoph and Oldham, Collin},
  title = {Practical Hardware and Algorithms for Creating Haptic Musical Instruments},
  pages = {61--66},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179495},
  url = {http://www.nime.org/proceedings/2008/nime2008_061.pdf},
  keywords = {haptic, actuator, practical, immersion, embedded, sampling rate, woofer, haptic drum, Cellomobo },
  abstract = {The music community has long had a strong interest in haptic technology. Recently, more effort has been put into making it more and more accessible to instrument designers.This paper covers some of these technologies with the aimof helping instrument designers add haptic feedback to theirinstruments. We begin by giving a brief overview of practicalactuators. Next, we compare and contrast using embeddedmicrocontrollers versus general purpose computers as controllers. Along the way, we mention some common softwareenvironments for implementing control algorithms. Then wediscuss the fundamental haptic control algorithms as well assome more complex ones. Finally, we present two practicaland effective haptic musical instruments: the haptic drumand the Cellomobo.}
}

@inproceedings{Zoran2008,
  author = {Zoran, Amit and Maes, Pattie},
  title = {Considering Virtual \& Physical Aspects in Acoustic Guitar Design},
  pages = {67--70},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177463},
  url = {http://www.nime.org/proceedings/2008/nime2008_067.pdf},
  keywords = {nime08}
}

@inproceedings{Menzies2008,
  author = {Menzies, Dylan},
  title = {Virtual Intimacy : Phya as an Instrument},
  pages = {71--76},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179599},
  url = {http://www.nime.org/proceedings/2008/nime2008_071.pdf},
  keywords = {NIME, musical expression, virtual reality, physical model- ing, audio synthesis },
  abstract = {Phya is an open source C++ library originally designed foradding physically modeled contact sounds into computergame environments equipped with physics engines. We review some aspects of this system, and also consider it fromthe purely aesthetic perspective of musical expression.}
}

@inproceedings{Butler2008,
  author = {Butler, Jennifer},
  title = {Creating Pedagogical Etudes for Interactive Instruments},
  pages = {77--80},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179503},
  url = {http://www.nime.org/proceedings/2008/nime2008_077.pdf},
  keywords = {composition,etudes,max,msp,musical controllers,musical expression,nime08,pedagogy,repertoire},
  abstract = {In this paper I discuss the importance of and need forpedagogical materials to support the development of newinterfaces and new instruments for electronic music. I describemy method for creating a graduated series of pedagogicaletudes composed using Max/MSP. The etudes will helpperformers and instrument designers learn the most commonlyused basic skills necessary to perform with interactiveelectronic music instruments. My intention is that the finalseries will guide a beginner from these initial steps through agraduated method, eventually incorporating some of the moreadvanced techniques regularly used by electronic musiccomposers.I describe the order of the series, and discuss the benefits (bothto performers and to composers) of having a logical sequence ofskill-based etudes. I also connect the significance of skilledperformers to the development of two essential areas that Iperceive are still just emerging in this field: the creation of acomposed repertoire and an increase in musical expressionduring performance.}
}

@inproceedings{Stowell2008,
  author = {Stowell, Dan and Plumbley, Mark D. and Bryan-Kinns, Nick},
  title = {Discourse Analysis Evaluation Method for Expressive Musical Interfaces},
  pages = {81--86},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179631},
  url = {http://www.nime.org/proceedings/2008/nime2008_081.pdf},
  keywords = {discourse analysis,evaluation,nime08,qualitative methods,voice},
  abstract = {The expressive and creative affordances of an interface aredifficult to evaluate, particularly with quantitative methods.However, rigorous qualitative methods do exist and can beused to investigate such topics. We present a methodologybased around user studies involving Discourse Analysis ofspeech. We also present an example of the methodologyin use: we evaluate a musical interface which utilises vocaltimbre, with a user group of beatboxers.}
}

@inproceedings{Kiefer2008,
  author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
  title = {HCI Methodology For Evaluating Musical Controllers : A Case Study},
  pages = {87--90},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179577},
  url = {http://www.nime.org/proceedings/2008/nime2008_087.pdf},
  keywords = {HCI Methodology, Wiimote, Evaluating Musical Interaction },
  abstract = {There is small but useful body of research concerning theevaluation of musical interfaces with HCI techniques. Inthis paper, we present a case study in implementing thesetechniques; we describe a usability experiment which evaluated the Nintendo Wiimote as a musical controller, andreflect on the effectiveness of our choice of HCI methodologies in this context. The study offered some valuable results,but our picture of the Wiimote was incomplete as we lackeddata concerning the participants' instantaneous musical experience. Recent trends in HCI are leading researchers totackle this problem of evaluating user experience; we reviewsome of their work and suggest that with some adaptation itcould provide useful new tools and methodologies for computer musicians.}
}

@inproceedings{Bau2008,
  author = {Bau, Olivier and Tanaka, Atau and Mackay, Wendy E.},
  title = {The A20 : Musical Metaphors for Interface Design},
  pages = {91--96},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179489},
  url = {http://www.nime.org/proceedings/2008/nime2008_091.pdf},
  keywords = {Generative design tools, Instrument building, Multi-faceted audio, Personal music devices, Tangible user interfaces, Technology probes },
  abstract = {We combine two concepts, the musical instrument as metaphorand technology probes, to explore how tangible interfaces canexploit the semantic richness of sound. Using participatorydesign methods from Human-Computer Interaction (HCI), wedesigned and tested the A20, a polyhedron-shaped, multichannel audio input/output device. The software maps soundaround the edges and responds to the user's gestural input,allowing both aural and haptic modes of interaction as well asdirect manipulation of media content. The software is designedto be very flexible and can be adapted to a wide range ofshapes. Our tests of the A20's perceptual and interactionproperties showed that users can successfully detect soundplacement, movement and haptic effects on this device. Ourparticipatory design workshops explored the possibilities of theA20 as a generative tool for the design of an extended,collaborative personal music player. The A20 helped users toenact scenarios of everyday mobile music player use and togenerate new design ideas.}
}

@inproceedings{Grosshauser2008,
  author = {Grosshauser, Tobias},
  title = {Low Force Pressure Measurement : Pressure Sensor Matrices for Gesture Analysis , Stiffness Recognition and Augmented Instruments},
  pages = {97--102},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179551},
  url = {http://www.nime.org/proceedings/2008/nime2008_097.pdf},
  keywords = {Pressure Measurement, Force, Sensor, Finger, Violin, Strings, Piano, Left Hand, Right Hand, Time Line, Cramping, Gesture and Posture Analysis. },
  abstract = {The described project is a new approach to use highly sensitive low force pressure sensor matrices for malposition, cramping and tension of hands and fingers, gesture and keystroke analysis and for new musical expression. In the latter, sensors are used as additional touch sensitive switches and keys. In pedagogical issues, new ways of technology enhanced teaching, self teaching and exercising are described. The used sensors are custom made in collaboration with the ReactiveS Sensorlab. }
}

@inproceedings{Torre2008,
  author = {Torre, Giuseppe and Torres, Javier and Fernstr\''{o}m, Mikael},
  title = {The Development of Motion Tracking Algorithms for Low Cost Inertial Measurement Units},
  pages = {103--106},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179641},
  url = {http://www.nime.org/proceedings/2008/nime2008_103.pdf},
  keywords = {eu-,ler,max,micro-electro-mechanical,msp,nime08,orientation matrix,pd,pitch yaw and roll,quaternion,sensors,surement unit,tracking orientation,wimu,wireless inertial mea-},
  abstract = {In this paper, we describe an algorithm for the numericalevaluation of the orientation of an object to which a clusterof accelerometers, gyroscopes and magnetometers has beenattached. The algorithm is implemented through a set ofMax/Msp and pd new externals. Through the successfulimplementation of the algorithm, we introduce Pointingat, a new gesture device for the control of sound in a 3Denvironment. This work has been at the core of the Celeritas Project, an interdisciplinary research project on motiontracking technology and multimedia live performances between the Tyndall Institute of Cork and the InteractionDesign Centre of Limerick.}
}

@inproceedings{Freed2008,
  author = {Freed, Adrian},
  title = {Application of new Fiber and Malleable Materials for Agile Development of Augmented Instruments and Controllers},
  pages = {107--112},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179539},
  url = {http://www.nime.org/proceedings/2008/nime2008_107.pdf},
  keywords = {Agile Development, Rapid Prototyping, Conductive fabric, Piezoresistive fabric, conductive heatshrink tubing, augmented instruments. },
  abstract = {The paper introduces new fiber and malleable materials,including piezoresistive fabric and conductive heat-shrinktubing, and shows techniques and examples of how they maybe used for rapid prototyping and agile development of musicalinstrument controllers. New implementations of well-knowndesigns are covered as well as enhancements of existingcontrollers. Finally, two new controllers are introduced that aremade possible by these recently available materials andconstruction techniques.}
}

@inproceedings{Crevoisier2008,
  author = {Crevoisier, Alain and Kellum, Greg},
  title = {Transforming Ordinary Surfaces into Multi-touch Controllers},
  pages = {113--116},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179517},
  url = {http://www.nime.org/proceedings/2008/nime2008_113.pdf},
  keywords = {Computer Vision, Multi-touch Interaction, Musical Interfaces. },
  abstract = {In this paper, we describe a set of hardware and software tools for creating musical controllers with any flat surface or simple object, such as tables, walls, metallic plates, wood boards, etc. The system makes possible to transform such physical objects and surfaces into virtual control interfaces, by using computer vision technologies to track the interaction made by the musician, either with the hands, mallets or sticks. These new musical interfaces, freely reconfigurable, can be used to control standard sound modules or effect processors, by defining zones on their surface and assigning them musical commands, such as the triggering of notes or the modulation of parameters.}
}

@inproceedings{Ward2008,
  author = {Ward, Nicholas and Penfield, Kedzie and O'Modhrain, Sile and Knapp, Benjamin},
  title = {A Study of Two Thereminists : Towards Movement Informed Instrument Design},
  pages = {117--121},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179649},
  url = {http://www.nime.org/proceedings/2008/nime2008_117.pdf},
  keywords = {Effort Phrasing, Recuperation, Laban Movement Analysis, Theremin },
  abstract = {This paper presents a comparison of the movement styles of two theremin players based on observation and analysis of video recordings. The premise behind this research is that a consideration of musicians' movements could form the basis for a new framework for the design of new instruments. Laban Movement Analysis is used to qualitatively analyse the movement styles of the musicians and to argue that the Recuperation phase of their phrasing is essential to achieve satisfactory performance. }
}

@inproceedings{Maniatakos2008,
  author = {Maniatakos, Vassilios-Fivos A. and Jacquemin, Christian},
  title = {Towards an Affective Gesture Interface for Expressive Music Performance},
  pages = {122--127},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179595},
  url = {http://www.nime.org/proceedings/2008/nime2008_122.pdf},
  keywords = {affective computing, interactive performance, HMM, gesture recognition, intelligent mapping, affective interface }
}

@inproceedings{Kallblad2008,
  author = {K\''{a}llblad, Anna and Friberg, Anders and Svensson, Karl and Edelholm, Elisabet S.},
  title = {Hoppsa Universum -- An Interactive Dance Installation for Children},
  pages = {128--133},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179573},
  url = {http://www.nime.org/proceedings/2008/nime2008_128.pdf},
  keywords = {Installation, dance, video recognition, children's movement, interactive multimedia },
  abstract = {It started with an idea to create an empty space in which you activated music and light as you moved around. In responding to the music and lighting you would activate more or different sounds and thereby communicate with the space through your body. This led to an artistic research project in which children's spontaneous movement was observed, a choreography made based on the children's movements and music written and recorded for the choreography. This music was then decomposed and choreographed into an empty space at Botkyrka konsthall creating an interactive dance installation. It was realized using an interactive sound and light system in which 5 video cameras were detecting the motion in the room connected to a 4-channel sound system and a set of 14 light modules. During five weeks people of all ages came to dance and move around in the installation. The installation attracted a wide range of people of all ages and the tentative evaluation indicates that it was very positively received and that it encouraged free movement in the intended way. Besides observing the activity in the installation interviews were made with schoolchildren age 7 who had participated in the installation. }
}

@inproceedings{Camurri2008,
  author = {Camurri, Antonio and Canepa, Corrado and Coletta, Paolo and Mazzarino, Barbara and Volpe, Gualtiero},
  title = {Mappe per Affetti Erranti : a Multimodal System for Social Active Listening and Expressive Performance},
  pages = {134--139},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179505},
  url = {http://www.nime.org/proceedings/2008/nime2008_134.pdf},
  keywords = {Active listening of music, expressive interfaces, full-body motion analysis and expressive gesture processing, multimodal interactive systems for music and performing arts applications, collaborative environments, social interaction. }
}

@inproceedings{Canazza2008,
  author = {Canazza, Sergio and Dattolo, Antonina},
  title = {New Data Structure for Old Musical Open Works},
  pages = {140--143},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179507},
  url = {http://www.nime.org/proceedings/2008/nime2008_140.pdf},
  keywords = {Musical Open Work, Multimedia Information Systems, Software Agents, zz-structures. },
  abstract = {Musical open works can be often thought like sequences of musical structures, which can be arranged by anyone who had access to them and who wished to realize the work. This paper proposes an innovative agent-based system to model the information and organize it in structured knowledge; to create effective, graph-centric browsing perspectives and views for the user; to use ,
,
authoring tools for the performance of open work of electro-acoustic music. }
}

@inproceedings{Eigenfeldt2008,
  author = {Eigenfeldt, Arne and Kapur, Ajay},
  title = {An Agent-based System for Robotic Musical Performance},
  pages = {144--149},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179527},
  url = {http://www.nime.org/proceedings/2008/nime2008_144.pdf},
  keywords = {Robotic Musical Instruments, Agents, Machine Musicianship. },
  abstract = {This paper presents an agent-based architecture for robotic musical instruments that generate polyphonic rhythmic patterns that continuously evolve and develop in a musically "intelligent" manner. Agent-based software offers a new method for real-time composition that allows for complex interactions between individual voices while requiring very little user interaction or supervision. The system described, Kinetic Engine, is an environment in which individual software agents, emulate drummers improvising within a percussion ensemble. Player agents assume roles and personalities within the ensemble, and communicate with one another to create complex rhythmic interactions. In this project, the ensemble is comprised of a 12-armed musical robot, MahaDeviBot, in which each limb has its own software agent controlling what it performs. }
}

@inproceedings{Goina2008,
  author = {Goina, Maurizio and Polotti, Pietro},
  title = {Elementary Gestalts for Gesture Sonification},
  pages = {150--153},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179549},
  url = {http://www.nime.org/proceedings/2008/nime2008_150.pdf},
  keywords = {Bauhaus, Klee, gesture analysis, sonification. },
  abstract = {In this paper, we investigate the relationships between gesture and sound by means of an elementary gesture sonification. This work takes inspiration from Bauhaus' ideals and Paul Klee's investigation into forms and pictorial representation. In line with these ideas, the main aim of this work is to reduce gesture to a combination of a small number of elementary components (gestalts) used to control a corresponding small set of sounds. By means of a demonstrative tool, we introduce here a line of research that is at its initial stage. The envisaged goal of future developments is a novel system that could be a composing/improvising tool as well as an interface for interactive dance and performance. }
}

@inproceedings{DelleMonache2008,
  author = {Delle Monache, Stefano and Polotti, Pietro and Papetti, Stefano and Rocchesso, Davide},
  title = {Sonically Augmented Found Objects},
  pages = {154--157},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179519},
  url = {http://www.nime.org/proceedings/2008/nime2008_154.pdf},
  keywords = {Rag-time washboard, sounding objects, physics-based sound synthesis, interactivity, sonification, augmented everyday objects. },
  abstract = {We present our work with augmented everyday objectstransformed into sound sources for music generation. The idea isto give voice to objects through technology. More specifically, theparadigm of the birth of musical instruments as a sonification ofobjects used in domestic or work everyday environments is hereconsidered and transposed into the technologically augmentedscenarios of our contemporary world.}
}

@inproceedings{Pelletier2008,
  author = {Pelletier, Jean-Marc},
  title = {Sonified Motion Flow Fields as a Means of Musical Expression},
  pages = {158--163},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179611},
  url = {http://www.nime.org/proceedings/2008/nime2008_158.pdf},
  keywords = {Computer vision, control field, image analysis, imaging, mapping, microsound, motion flow, sonification, synthesis },
  abstract = {This paper describes a generalized motion-based framework forthe generation of large musical control fields from imaging data.The framework is general in the sense that it does not depend ona particular source of sensing data. Real-time images of stageperformers, pre-recorded and live video, as well as more exoticdata from imaging systems such as thermography, pressuresensor arrays, etc. can be used as a source of control. Featurepoints are extracted from the candidate images, from whichmotion vector fields are calculated. After some processing, thesemotion vectors are mapped individually to sound synthesisparameters. Suitable synthesis techniques include granular andmicrosonic algorithms, additive synthesis and micro-polyphonicorchestration. Implementation details of this framework isdiscussed, as well as suitable creative and artistic uses andapproaches.}
}

@inproceedings{Dubrau2008,
  author = {Dubrau, Josh and Havryliv, Mark},
  title = {P[a]ra[pra]xis : Poetry in Motion},
  pages = {164--167},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179525},
  url = {http://www.nime.org/proceedings/2008/nime2008_164.pdf},
  keywords = {Poetry, language sonification, psychoanalysis, linguistics, Freud, realtime poetry. }
}

@inproceedings{Schacher2008,
  author = {Schacher, Jan C.},
  title = {Davos Soundscape, a Location Based Interactive Composition},
  pages = {168--171},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179623},
  url = {http://www.nime.org/proceedings/2008/nime2008_168.pdf},
  keywords = {Location-based, electronic music, composition, embedded Linux, GPS, Pure Data, interaction, mapping, soundscape },
  abstract = {Moving out of doors with digital tools and electronic music and creating musically rich experiences is made possible by the increased availability of ever smaller and more powerful mobile computers. Composing music for and in a landscape instead of for a closed architectural space offers new perspectives but also raises questions about interaction and composition of electronic music. The work we present here was commissioned by a festival and ran on a daily basis over a period of three months. A GPS-enabled embedded Linux system is assembled to serve as a location-aware sound platform. Several challenges have to be overcome both technically and artistically to achieve a seamless experience and provide a simple device to be handed to the public. By building this interactive experience, which relies as much on the user's willingness to explore the invisible sonic landscape as on the ability to deploy the technology, a number of new avenues for exploring electronic music and interactivity in location-based media open up. New ways of composing music for and in a landscape and for creating audience interaction are explored. }
}

@inproceedings{Schmeder2008,
  author = {Schmeder, Andrew and Freed, Adrian},
  title = {uOSC : The Open Sound Control Reference Platform for Embedded Devices},
  pages = {175--180},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179627},
  url = {http://www.nime.org/proceedings/2008/nime2008_175.pdf},
  keywords = {jitter,latency,nime08,open sound control,pic microcontroller,usb},
  abstract = {A general-purpose firmware for a low cost microcontroller is described that employs the Open Sound Control protocol over USB. The firmware is designed with considerations for integration in new musical interfaces and embedded devices. Features of note include stateless design, efficient floating-point support, temporally correct data handling, and protocol completeness. A timing performance analysis is conducted.}
}

@inproceedings{Place2008,
  author = {Place, Timothy and Lossius, Trond and Jensenius, Alexander R. and Peters, Nils},
  title = {Addressing Classes by Differentiating Values and Properties in OSC},
  pages = {181--184},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179613},
  url = {http://www.nime.org/proceedings/2008/nime2008_181.pdf},
  keywords = {jamoma,namespace,nime08,osc,standardization},
  abstract = {An approach for creating structured Open Sound Control(OSC) messages by separating the addressing of node valuesand node properties is suggested. This includes a methodfor querying values and properties. As a result, it is possibleto address complex nodes as classes inside of more complextree structures using an OSC namespace. This is particularly useful for creating flexible communication in modularsystems. A prototype implementation is presented and discussed.}
}

@inproceedings{Platz2008,
  author = {Ananya, Misra and Essl, Georg and Rohs, Michael},
  title = {Microphone as Sensor in Mobile Phone Performance},
  pages = {185--188},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179485},
  url = {http://www.nime.org/proceedings/2008/nime2008_185.pdf},
  keywords = {mobile music making, microphone, mobile-stk },
  abstract = {Many mobile devices, specifically mobile phones, come equipped with a microphone. Microphones are high-fidelity sensors that can pick up sounds relating to a range of physical phenomena. Using simple feature extraction methods,parameters can be found that sensibly map to synthesis algorithms to allow expressive and interactive performance.For example blowing noise can be used as a wind instrument excitation source. Also other types of interactionscan be detected via microphones, such as striking. Hencethe microphone, in addition to allowing literal recording,serves as an additional source of input to the developingfield of mobile phone performance.}
}

@inproceedings{Bouillot2008,
  author = {Bouillot, Nicolas and Wozniewski, Mike and Settel, Zack and Cooperstock, Jeremy R.},
  title = {A Mobile Wireless Augmented Guitar},
  pages = {189--192},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179499},
  url = {http://www.nime.org/proceedings/2008/nime2008_189.pdf},
  keywords = {nime08}
}

@inproceedings{Jacobs2008,
  author = {Jacobs, Robert and Feldmeier, Mark and Paradiso, Joseph A.},
  title = {A Mobile Music Environment Using a PD Compiler and Wireless Sensors},
  pages = {193--196},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179567},
  url = {http://www.nime.org/proceedings/2008/nime2008_193.pdf},
  keywords = {None},
  abstract = {None}
}

@inproceedings{Bencina2008,
  author = {Bencina, Ross and Wilde, Danielle and Langley, Somaya},
  title = {Gesture=Sound Experiments : Process and Mappings},
  pages = {197--202},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179491},
  url = {http://www.nime.org/proceedings/2008/nime2008_197.pdf},
  keywords = {gestural control,mapping,nime08,prototyping,three-axis accelerometers,vocal,wii remote}
}

@inproceedings{Ciglar2008,
  author = {Ciglar, Miha},
  title = {"3rd. Pole" -- A Composition Performed via Gestural Cues},
  pages = {203--206},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179511},
  url = {http://www.nime.org/proceedings/2008/nime2008_203.pdf},
  keywords = {dancer, fig, from the system in, gesture recognition, haptic feedback, in, markers attached to the, motion tracking, nime08, s limbs, the dancer receives feedback, two ways}
}

@inproceedings{Hansen2008,
  author = {Hansen, Kjetil F. and Alonso, Marcos},
  title = {More DJ Techniques on the reactable},
  pages = {207--210},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179555},
  url = {http://www.nime.org/proceedings/2008/nime2008_207.pdf},
  keywords = {dj scratch techniques,interfaces,nime08,playability,reactable},
  abstract = {This paper describes a project started for implementing DJscratching techniques on the reactable. By interacting withobjects representing scratch patterns commonly performedon the turntable and the crossfader, the musician can playwith DJ techniques and manipulate how they are executedin a performance. This is a novel approach to the digital DJapplications and hardware. Two expert musicians practisedand performed on the reactable in order to both evaluate theplayability and improve the design of the DJ techniques.}
}

@inproceedings{Dimitrov2008,
  author = {Dimitrov, Smilen and Alonso, Marcos and Serafin, Stefania},
  title = {Developing Block-Movement, Physical-Model Based Objects for the Reactable},
  pages = {211--214},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179523},
  url = {http://www.nime.org/proceedings/2008/nime2008_211.pdf},
  keywords = {Reactable, physical model, motion sonification, contact fric- tion },
  abstract = {This paper reports on a Short-Term Scientific Mission (STSM)sponsored by the Sonic Interaction Design (SID) EuropeanCOST Action IC601.Prototypes of objects for the novel instrument Reactablewere developed, with the goal of studying sonification ofmovements on this platform using physical models. A physical model of frictional interactions between rubbed dry surfaces was used as an audio generation engine, which alloweddevelopment in two directions --- a set of objects that affordsmotions similar to sliding, and a single object aiming tosonify contact friction sound. Informal evaluation was obtained from a Reactable expert user, regarding these sets ofobjects. Experiments with the objects were also performed- related to both audio filtering, and interfacing with otherobjects for the Reactable.}
}

@inproceedings{Thiebaut2008,
  author = {Thiebaut, Jean-Baptiste and Abdallah, Samer and Robertson, Andrew and Bryan-Kinns, Nick and Plumbley, Mark D.},
  title = {Real Time Gesture Learning and Recognition : Towards Automatic Categorization},
  pages = {215--218},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179639},
  url = {http://www.nime.org/proceedings/2008/nime2008_215.pdf},
  keywords = {Gesture recognition, supervised and unsupervised learning, interaction, haptic feedback, information dynamics, HMMs },
  abstract = {This research focuses on real-time gesture learning and recognition. Events arrive in a continuous stream without explicitly given boundaries. To obtain temporal accuracy, weneed to consider the lag between the detection of an eventand any effects we wish to trigger with it. Two methodsfor real time gesture recognition using a Nintendo Wii controller are presented. The first detects gestures similar to agiven template using either a Euclidean distance or a cosinesimilarity measure. The second method uses novel information theoretic methods to detect and categorize gestures inan unsupervised way. The role of supervision, detection lagand the importance of haptic feedback are discussed.}
}

@inproceedings{Kimura2008,
  author = {Kimura, Mari},
  title = {Making of VITESSIMO for Augmented Violin : Compositional Process and Performance},
  pages = {219--220},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179581},
  url = {http://www.nime.org/proceedings/2008/nime2008_219.pdf},
  keywords = {Augmented Violin, gesture tracking, interactive performance },
  abstract = {This paper describes the compositional process for creatingthe interactive work for violin entitled VITESSIMO using theAugmented Violin [1].}
}

@inproceedings{Loviscach2008,
  author = {Loviscach, J\''{o}rn},
  title = {Programming a Music Synthesizer through Data Mining},
  pages = {221--224},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179591},
  url = {http://www.nime.org/proceedings/2008/nime2008_221.pdf},
  keywords = {Information visualization, mutual information, intelligent user interfaces },
  abstract = {Sound libraries for music synthesizers easily comprise one thousand or more programs (''patches''). Thus, there are enough raw data to apply data mining to reveal typical settings and to extract dependencies. Intelligent user interfaces for music synthesizers can be based on such statistics. This paper proposes two approaches: First, the user sets any number of parameters and then lets the system find the nearest sounds in the database, a kind of patch autocompletion. Second, all parameters are "live" as usual, but turning one knob or setting a switch will also change the settings of other, statistically related controls. Both approaches canbe used with the standard interface of the synthesizer. On top of that, this paper introduces alternative or additional interfaces based on data visualization.}
}

@inproceedings{Ng2008,
  author = {Ng, Kia and Nesi, Paolo},
  title = {i-Maestro : Technology-Enhanced Learning and Teaching for Music},
  pages = {225--228},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179605},
  url = {http://www.nime.org/proceedings/2008/nime2008_225.pdf},
  keywords = {augmented instrument,education,gesture,interactive,interface,motion,multimedia,music,nime08,notation,sensor,sonification,technology-enhanced learning,visualisation},
  abstract = {This paper presents a project called i-Maestro (www.i-maestro.org) which develops interactive multimedia environments for technology enhanced music education. The project explores novel solutions for music training in both theory and performance, building on recent innovations resulting from the development of computer and information technologies, by exploiting new pedagogical paradigms with cooperative and interactive self-learning environments, gesture interfaces, and augmented instruments. This paper discusses the general context along with the background and current developments of the project, together with an overview of the framework and discussions on a number of selected tools to support technology-enhanced music learning and teaching. }
}

@inproceedings{Kuyken2008,
  author = {Kuyken, Bart and Verstichel, Wouter and Bossuyt, Frederick and Vanfleteren, Jan and Demey, Michiel and Leman, Marc},
  title = {The HOP Sensor : Wireless Motion Sensor},
  pages = {229--232},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179583},
  url = {http://www.nime.org/proceedings/2008/nime2008_229.pdf},
  keywords = {Digital Musical Instrument, Wireless Sensors, Inertial Sensing, Hop Sensor },
  abstract = {This paper describes the HOP system. It consists of a wireless module built up by multiple nodes and a base station. The nodes detect acceleration of e.g. human movement. At a rate of 100 Hertz the base station collects the acceleration samples. The data can be acquired in real-time software like Pure Data and Max/MSP. The data can be used to analyze and/or sonify movement. }
}

@inproceedings{Coghlan2008,
  author = {Coghlan, Niall and Knapp, Benjamin},
  title = {Sensory Chairs : A System for Biosignal Research and Performance},
  pages = {233--236},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179513},
  url = {http://www.nime.org/proceedings/2008/nime2008_233.pdf},
  keywords = {Ubiquitous computing, context -awareness, networking, embedded systems, chairs, digital artefacts, emotional state sensing, affective computing, biosignals. }
}

@inproceedings{Godbehere2008,
  author = {Godbehere, Andrew B. and Ward, Nathan J.},
  title = {Wearable Interfaces for Cyberphysical Musical Expression},
  pages = {237--240},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179547},
  url = {http://www.nime.org/proceedings/2008/nime2008_237.pdf},
  keywords = {Wearable computing, personal area networks, accelerometers, 802.15.4, motion analysis, human-computer interaction, live performance, digital musical controllers, gestural control },
  abstract = {We present examples of a wireless sensor network as applied to wearable digital music controllers. Recent advances in wireless Personal Area Networks (PANs) have precipitated the IEEE 802.15.4 standard for low-power, low-cost wireless sensor networks. We have applied this new technology to create a fully wireless, wearable network of accelerometers which are small enough to be hidden under clothing. Various motion analysis and machine learning techniques are applied to the raw accelerometer data in real-time to generate and control music on the fly. }
}

@inproceedings{Hayafuchi2008,
  author = {Hayafuchi, Kouki and Suzuki, Kenji},
  title = {MusicGlove: A Wearable Musical Controller for Massive Media Library},
  pages = {241--244},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179561},
  url = {http://www.nime.org/proceedings/2008/nime2008_241.pdf},
  keywords = {Embodied Sound Media, Music Controller, Gestures, Body Motion, Musical Interface },
  abstract = {This research aims to develop a wearable musical interface which enables to control audio and video signals by using hand gestures and human body motions. We have been developing an audio-visual manipulation system that realizes tracks control, time-based operations and searching for tracks from massive music library. It aims to build an emotional and affecting musical interaction, and will provide a better method of music listening to people. A sophisticated glove-like device with an acceleration sensor and several strain sensors has been developed. A realtime signal processing and musical control are executed as a result of gesture recognition. We also developed a stand-alone device that performs as a musical controller and player at the same time. In this paper, we describe the development of a compact and sophisticated sensor device, and demonstrate its performance of audio and video signals control.}
}

@inproceedings{Zbyszynski2008,
  author = {Zbyszynski, Michael},
  title = {An Elementary Method for Tablet},
  pages = {245--248},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177461},
  url = {http://www.nime.org/proceedings/2008/nime2008_245.pdf},
  keywords = {Wacom tablet, digitizing tablet, expressivity, gesture, mapping, pedagogy, practice },
  abstract = {This paper proposes the creation of a method book for tabletbased instruments, evaluating pedagogical materials fortraditional instruments as well as research in human-computerinteraction and tablet interfaces.}
}

@inproceedings{Roma2008,
  author = {Roma, Gerard and Xamb\'{o}, Anna},
  title = {A Tabletop Waveform Editor for Live Performance},
  pages = {249--252},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179621},
  url = {http://www.nime.org/proceedings/2008/nime2008_249.pdf},
  keywords = {tangible interface, tabletop interface, musical performance, interaction techniques },
  abstract = {We present an audio waveform editor that can be operated in real time through a tabletop interface. The systemcombines multi-touch and tangible interaction techniques inorder to implement the metaphor of a toolkit that allows direct manipulation of a sound sample. The resulting instrument is well suited for live performance based on evolvingloops.}
}

@inproceedings{Valle2008a,
  author = {Valle, Andrea},
  title = {Integrated Algorithmic Composition Fluid systems for including notation in music composition cycle},
  pages = {253--256},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179645},
  url = {http://www.nime.org/proceedings/2008/nime2008_253.pdf},
  keywords = {algorithmic composition,automatic notation,nime08}
}

@inproceedings{Valle2008,
  author = {Valle, Andrea},
  title = {GeoGraphy : a Real-Time, Graph-Based Composition Environment},
  pages = {257--260},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179643},
  url = {http://www.nime.org/proceedings/2008/nime2008_257.pdf},
  keywords = {a graph,composition,figure 1,interfaces,left,live coding,musical algorithmic composition,nime08,performance,vertex durations and coor-},
  abstract = {This paper is about GeoGraphy, a graph-based system forthe control of both musical composition and interactive performance and its implementation in a real-time, interactiveapplication. The implementation includes a flexible userinterface system.}
}

@inproceedings{Zannos2008,
  author = {Zannos, Iannis},
  title = {Multi-Platform Development of Audiovisual and Kinetic Installations},
  pages = {261--264},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1177459},
  url = {http://www.nime.org/proceedings/2008/nime2008_261.pdf},
  keywords = {kinetic art, audiovisual installations, python, SuperCollider, Processing, algorithmic art, tools for multi-platform development },
  abstract = {In this paper, we describe the development of multi-platform tools for Audiovisual and Kinetic installations. These involve the connection of three development environments: Python, SuperCollider and Processing, in order to drive kinetic art installations and to combine these with digital synthesis of sound and image in real time. By connecting these three platforms via the OSC protocol, we enable the control in real time of analog physical media (a device that draws figures on sand), sound synthesis and image synthesis. We worked on the development of algorithms for drawing figures and synthesizing images and sound on all three platforms and experimented with various mechanisms for coordinating synthesis and rendering in different media. Several problems were addressed: How to coordinate the timing between different platforms? What configuration to use? Clientserver (who is the client who the server?), equal partners, mixed configurations. A library was developed in SuperCollider to enable the packaging of algorithms into modules with automatic generation of GUI from specifications, and the saving of configurations of modules into session files as scripts in SuperCollider code. The application of this library as a framework for both driving graphic synthesis in Processing and receiving control data from it resulted in an environment for experimentation that is also being used successfully in teaching interactive audiovisual media. }
}

@inproceedings{Corness2008,
  author = {Corness, Greg},
  title = {Performer Model : Towards a Framework for Interactive Performance Based on Perceived Intention},
  pages = {265--268},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179515},
  url = {http://www.nime.org/proceedings/2008/nime2008_265.pdf},
  keywords = {Interactive performance, Perception, HCI },
  abstract = {Through the developing of tools for analyzing the performerssonic and movement-based gestures, research into the systemperformer interaction has focused on the computer's ability torespond to the performer. Where as such work shows interestwithin the community in developing an interaction paradigmmodeled on the player, by focusing on the perception andreasoning of the system, this research assumes that theperformer's manner of interaction is in agreement with thiscomputational model. My study presents an alternative model ofinteraction designed for improvisatory performance centered onthe perception of the performer as understood by theories takenfrom performance practices and cognitive science.}
}

@inproceedings{Teles2008,
  author = {Teles, Paulo C. and Boyle, Aidan},
  title = {Developing an "Antigenous" Art Installation Based on a Touchless Endosystem Interface},
  pages = {269--272},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179637},
  url = {http://www.nime.org/proceedings/2008/nime2008_269.pdf},
  keywords = {nime08}
}

@inproceedings{Lanzalone2008,
  author = {Lanzalone, Silvia},
  title = {The 'Suspended Clarinet' with the 'Uncaused Sound' : Description of a Renewed Musical Instrument},
  pages = {273--276},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179587},
  url = {http://www.nime.org/proceedings/2008/nime2008_273.pdf},
  keywords = {nime08}
}

@inproceedings{Hashida2008,
  author = {Hashida, Mitsuyo and Ito, Yosuke and Katayose, Haruhiro},
  title = {A Directable Performance Rendering System: Itopul},
  pages = {277--280},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179559},
  url = {http://www.nime.org/proceedings/2008/nime2008_277.pdf},
  keywords = {Performance Rendering, User Interface, Case-based Approach },
  abstract = {One of the advantages of case-based systems is that theycan generate expressions even if the user doesn't know howthe system applies expression rules. However, the systemscannot avoid the problem of data sparseness and do notpermit a user to improve the expression of a certain part ofa melody directly. After discussing the functions requiredfor user-oriented interface for performance rendering systems, this paper proposes a directable case-based performance rendering system, called Itopul. Itopul is characterized by 1) a combination of the phrasing model and thepulse model, 2) the use of a hierarchical music structure foravoiding from the data sparseness problem, 3) visualizationof the processing progress, and 4) music structures directlymodifiable by the user.}
}

@inproceedings{Hazlewood2008,
  author = {Hazlewood, William R. and Knopke, Ian},
  title = {Designing Ambient Musical Information Systems},
  pages = {281--284},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179563},
  url = {http://www.nime.org/proceedings/2008/nime2008_281.pdf},
  keywords = {Ambient Musical Information Systems, musical instruments, human computer interaction, Markov chain, probability, al- gorithmic composition },
  abstract = {In this work we describe our initial explorations in building a musical instrument specifically for providing listenerswith simple, but useful, ambient information. The termAmbient Musical Information Systems (AMIS) is proposedto describe this kind of research. Instruments like these differ from standard musical instruments in that they are tobe perceived indirectly from outside one's primary focus ofattention. We describe our rationale for creating such a device, a discussion on the appropriate qualities of sound fordelivering ambient information, and a description of an instrument created for use in a series of experiments that wewill use to test out ideas. We conclude with a discussion ofour initial findings, and some further directions we wish toexplore.}
}

@inproceedings{Hadjakos2008,
  author = {Hadjakos, Aristotelis and Aitenbichler, Erwin and M\''{u}hlh\''{a}user, Max},
  title = {The Elbow Piano : Sonification of Piano Playing Movements},
  pages = {285--288},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179553},
  url = {http://www.nime.org/proceedings/2008/nime2008_285.pdf},
  keywords = {Piano, education, sonification, feedback, gesture. },
  abstract = {The Elbow Piano distinguishes two types of piano touch: a touchwith movement in the elbow joint and a touch without. A playednote is first mapped to the left or right hand by visual tracking.Custom-built goniometers attached to the player's arms are usedto detect the type of touch. The two different types of touchesare sonified by different instrument sounds. This gives theplayer an increased awareness of his elbow movements, which isconsidered valuable for piano education. We have implementedthe system and evaluated it with a group of music students.}
}

@inproceedings{Takegawa2008,
  author = {Takegawa, Yoshinari and Tsukamoto, Masahiko},
  title = {UnitKeyboard : An Easily Configurable Compact Clavier},
  pages = {289--292},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179635},
  url = {http://www.nime.org/proceedings/2008/nime2008_289.pdf},
  keywords = {Portable keyboard instruments, block interface, Automatic settings },
  abstract = {Musical keyboard instruments have a long history, whichresulted in many kinds of keyboards (claviers) today. Sincethe hardware of conventional musical keyboards cannot bechanged, such as the number of keys, musicians have tocarry these large keyboards for playing music that requiresonly a small diapason. To solve this problem, the goal ofour study is to construct UnitKeyboard, which has only 12keys (7 white keys and 5 black keys) and connectors fordocking with other UnitKeyboards. We can build variouskinds of musical keyboard configurations by connecting oneUnitKeyboard to others, since they have automatic settingsfor multiple keyboard instruments. We discuss the usabilityof the UnitKeyboard from reviews by several amateur andprofessional pianists who used the UnitKeyboard.}
}

@inproceedings{PalacioQuintin2008,
  author = {Palacio-Quintin, Cl\'{e}o},
  title = {Eight Years of Practice on the Hyper-Flute : Technological and Musical Perspectives},
  pages = {293--298},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179609},
  url = {http://www.nime.org/proceedings/2008/nime2008_293.pdf},
  keywords = {composition,gestural control,hyper-flute,hyper-instruments,improvisation,interactive music,mapping,nime08,sensors},
  abstract = {After eight years of practice on the first hyper-flute prototype (a flute extended with sensors), this article presentsa retrospective of its instrumental practice and the newdevelopments planned from both technological and musical perspectives. Design, performance skills, and mappingstrategies are discussed, as well as interactive compositionand improvisation.}
}

@inproceedings{Berdahl2008,
  author = {Berdahl, Edgar and Smith, Julius O.},
  title = {A Tangible Virtual Vibrating String : A Physically Motivated Virtual Musical Instrument Interface},
  pages = {299--302},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179493},
  url = {http://www.nime.org/proceedings/2008/nime2008_299.pdf},
  keywords = {physically motivated, physical, models, modeling, vibrating string, guitar, pitch detection, interface, excitation, coupled strings, haptic },
  abstract = {We introduce physically motivated interfaces for playing virtual musical instruments, and we suggest that they lie somewhere in between commonplace interfaces and haptic interfaces in terms of their complexity. Next, we review guitarlike interfaces, and we design an interface to a virtual string.The excitation signal and pitch are sensed separately usingtwo independent string segments. These parameters controla two-axis digital waveguide virtual string, which modelsvibrations in the horizontal and vertical transverse axes aswell as the coupling between them. Finally, we consider theadvantages of using a multi-axis pickup for measuring theexcitation signal.}
}

@inproceedings{Geiger2008,
  author = {Geiger, Christian and Reckter, Holger and Paschke, David and Schulz, Florian and Poepel, Cornelius},
  title = {Towards Participatory Design and Evaluation of Theremin-based Musical Interfaces},
  pages = {303--306},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179545},
  url = {http://www.nime.org/proceedings/2008/nime2008_303.pdf},
  keywords = {3d interaction techniques,an important concept for,both hands,evaluation,few wimp interface concepts,in contrast the use,make efficient use of,nime08,of both hands is,theremin-based interfaces},
  abstract = {Being one of the earliest electronic instruments the basic principles of the Theremin have often been used to design new musical interfaces. We present the structured design and evaluation of a set of 3D interfaces for a virtual Theremin, the VRemin. The variants differ in the size of the interaction space, the interface complexity, and the applied IO devices. We conducted a formal evaluation based on the well-known AttrakDiff questionnaire for evaluating the hedonic and pragmatic quality of interactive products. The presented work is a first approach towards a participatory design process for musical interfaces that includes user evaluation at early design phases. }
}

@inproceedings{Henriques2008,
  author = {Henriques, Tom\'{a}s},
  title = {META-{EV}I Innovative Performance Paths with a Wind Controller},
  pages = {307--310},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179565},
  url = {http://www.nime.org/proceedings/2008/nime2008_307.pdf},
  keywords = {computer music,musical instrument,nime08,sensor technologies}
}

@inproceedings{Price2008,
  author = {Price, Robin and Rebelo, Pedro},
  title = {Database and Mapping Design for Audiovisual Prepared Radio Set Installation},
  pages = {311--314},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179615},
  url = {http://www.nime.org/proceedings/2008/nime2008_311.pdf},
  keywords = {Mapping, database, audiovisual, radio, installation art. }
}

@inproceedings{Jo2008,
  author = {Jo, Kazuhiro and Nagano, Norihisa},
  title = {Monalisa : "See the Sound , Hear the Image"},
  pages = {315--318},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179569},
  url = {http://www.nime.org/proceedings/2008/nime2008_315.pdf},
  keywords = {Sound and Image Processing Software, Plug-in, Installation },
  abstract = {Monalisa is a software platform that enables to "see the sound, hear the image". It consists of three software: Monalisa Application, Monalisa-Audio Unit, and Monalisa-Image Unit, and an installation: Monalisa "shadow of the sound". In this paper, we describe the implementation of each software and installation with the explanation of the basic algorithms to treat the image data and the sound data transparently.}
}

@inproceedings{Robertson2008,
  author = {Robertson, Andrew and Plumbley, Mark D. and Bryan-Kinns, Nick},
  title = {A Turing Test for B-Keeper : Evaluating an Interactive},
  pages = {319--324},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179619},
  url = {http://www.nime.org/proceedings/2008/nime2008_319.pdf},
  keywords = {Automatic Accompaniment, Beat Tracking, Human-Computer Interaction, Musical Interface Evaluation }
}

@inproceedings{Gatzsche2008,
  author = {Gatzsche, Gabriel and Mehnert, Markus and St\''{o}cklmeier, Christian},
  title = {Interaction with Tonal Pitch Spaces},
  pages = {325--330},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179541},
  url = {http://www.nime.org/proceedings/2008/nime2008_325.pdf},
  keywords = {Pitch space, musical interface, Carol L. Krumhansl, music psychology, music theory, western tonal music, 3D tonality model, spiral of thirds, 3D, Hardware controller, Symmetry model },
  abstract = {In this paper, we present a pitch space based musical interface approach. A pitch space arranges tones in a way that meaningful tone combinations can be easily generated. Using a touch sensitive surface or a 3D-Joystick a player can move through the pitch space and create the desired sound by selecting tones. The more optimal the tones are geometrically arranged, the less control parameters are required to move through the space and to select the desired pitches. For this the quality of pitch space based musical interfaces depends on two factors: 1. the way how the tones are organized within the pitch space and 2. the way how the parameters of a given controller are used to move through the space and to select pitches. This paper presents a musical interface based on a tonal pitch space derived from a four dimensional model found by the music psychologists [11], [2]. The proposed pitch space particularly eases the creation of tonal harmonic music. Simultaneously it outlines music psychological and theoretical principles of music. }
}

@inproceedings{Chordia2008,
  author = {Chordia, Parag and Rae, Alex},
  title = {Real-Time Raag Recognition for Interactive Music},
  pages = {331--334},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179509},
  url = {http://www.nime.org/proceedings/2008/nime2008_331.pdf},
  keywords = {automatic recognition,indian music,nime08,raag,raga},
  abstract = {We describe a system that can listen to a performance of Indian music and recognize the raag, the fundamental melodicframework that Indian classical musicians improvise within.In addition to determining the most likely raag being performed, the system displays the estimated the likelihoodof each of the other possible raags, visualizing the changesover time. The system computes the pitch-class distributionand uses a Bayesian decision rule to classify the resultingtwelve dimensional feature vector, where each feature represents the relative use of each pitch class. We show that thesystem achieves high performance on a variety of sources,making it a viable tool for interactive performance.}
}

@inproceedings{Vinjar2008,
  author = {Vinjar, Anders},
  title = {Bending Common Music with Physical Models},
  pages = {335--338},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179647},
  url = {http://www.nime.org/proceedings/2008/nime2008_335.pdf},
  keywords = {Physical Models in composition, CommonMusic, Musical mapping },
  abstract = {A general CAC1-environment charged with physical-modelling capabilities is described. It combines CommonMusic,ODE and Fluxus in a modular way, making a powerful andflexible environment for experimenting with physical modelsin composition.Composition in this respect refers to the generation andmanipulation of structure typically on or above a note, phrase or voice-level. Compared to efforts in synthesisand performance little work has gone into applying physicalmodels to composition. Potentials in composition-applications are presumably large.The implementation of the physically equipped CAC-environment is described in detail.}
}

@inproceedings{Schedel2008,
  author = {Schedel, Margaret and Rootberg, Alison and de Martelly, Elizabeth},
  title = {Scoring an Interactive, Multimedia Performance Work},
  pages = {339--342},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179625},
  url = {http://www.nime.org/proceedings/2008/nime2008_339.pdf},
  keywords = {nime08},
  abstract = {The Color of Waiting is an interactive theater workwith music, dance, and video which was developed atSTEIM in Amsterdam and further refined at CMMASin Morelia Mexico with funding from Meet theComposer. Using Max/MSP/ Jitter a cellist is able tocontrol sound and video during the performancewhile performing a structured improvisation inresponse to the dancer's movement. In order toensure. repeated performances of The Color o fWaiting , Kinesthetech Sense created the scorecontained in this paper. Performance is essential tothe practice of time-based art as a living form, buthas been complicated by the unique challenges ininterpretation and re-creation posed by worksincorporating technology. Creating a detailed scoreis one of the ways artists working with technologycan combat obsolescence.}
}

@inproceedings{Endo2008,
  author = {Endo, Ayaka and Kuhara, Yasuo},
  title = {Rhythmic Instruments Ensemble Simulator Generating Animation Movies Using {Bluetooth} Game Controller},
  pages = {345--346},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179529},
  url = {http://www.nime.org/proceedings/2008/nime2008_345.pdf},
  keywords = {Wii Remote, Wireless game controller, MIDI, Max/MSP, Flash movie, Gesture music and animation. },
  abstract = {We developed a rhythmic instruments ensemble simulator generating animation using game controllers. The motion of a player is transformed into musical expression data of MIDI to generate sounds, and MIDI data are transformed into animation control parameters to generate movies. These animations and music are shown as the reflection of player performance. Multiple players can perform a musical ensemble to make more varied patterns of animation. Our system is so easy that everyone can enjoy performing a fusion of music and animation. }
}

@inproceedings{McMillen2008,
  author = {McMillen, Keith A.},
  title = {Stage-Worthy Sensor Bows for Stringed Instruments},
  pages = {347--348},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179597},
  url = {http://www.nime.org/proceedings/2008/nime2008_347.pdf},
  keywords = {Sensor bow, stringed instruments, bluetooth },
  abstract = {The demonstration of a series of properly weighted and balanced Bluetooth sensor bows for violin, viola, cello and bass. }
}

@inproceedings{Flanigan2008,
  author = {Flanigan, Lesley and Doro, Andrew},
  title = {Plink Jet},
  pages = {349--351},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179533},
  url = {http://www.nime.org/proceedings/2008/nime2008_349.pdf},
  keywords = {Interaction Design, Repurposing of Consumer Technology, DIY, Performing Technology, Robotics, Automation, Infra-Instrument },
  abstract = {Plink Jet is a robotic musical instrument made from scavenged inkjet printers and guitar parts. We investigate the expressive capabilities of everyday machine technology by recontextualizing the relatively high-tech mechanisms of typical office debris into an electro-acoustic musical instrument. We also explore the performative relationship between human and machine.}
}

@inproceedings{Kamiyama2008,
  author = {Kamiyama, Yusuke and Tanaka, Mai and Tanaka, Hiroya},
  title = {Oto-Shigure : An Umbrella-Shaped Sound Generator for Musical Expression},
  pages = {352--353},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179575},
  url = {http://www.nime.org/proceedings/2008/nime2008_352.pdf},
  keywords = {umbrella, musical expression, sound generating device, 3D sound system, sound-field arrangement. }
}

@inproceedings{Follmer2008,
  author = {Follmer, Sean and Warren, Chris and Marquez-Borbon, Adnan},
  title = {The Pond : Interactive Multimedia Installation},
  pages = {354--355},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179535},
  url = {http://www.nime.org/proceedings/2008/nime2008_354.pdf},
  keywords = {nime08}
}

@inproceedings{Hartman2008,
  author = {Hartman, Ethan and Cooper, Jeff and Spratt, Kyle},
  title = {Swing Set : Musical Controllers with Inherent Physical Dynamics},
  pages = {356--357},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179557},
  url = {http://www.nime.org/proceedings/2008/nime2008_356.pdf},
  keywords = {nime08}
}

@inproceedings{Modler2008,
  author = {Modler, Paul and Myatt, Tony},
  title = {Video Based Recognition of Hand Gestures by Neural Networks for the Control of Sound and Music},
  pages = {358--359},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179601},
  url = {http://www.nime.org/proceedings/2008/nime2008_358.pdf},
  keywords = {nime08}
}

@inproceedings{Suzuki2008,
  author = {Suzuki, Kenji and Kyoya, Miho and Kamatani, Takahiro and Uchiyama, Toshiaki},
  title = {beacon : Embodied Sound Media Environment for Socio-Musical Interaction},
  pages = {360--361},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179633},
  url = {http://www.nime.org/proceedings/2008/nime2008_360.pdf},
  keywords = {Embodied sound media, Hyper-instrument, Laser beams },
  abstract = {This research aims to develop a novel instrument for sociomusical interaction where a number of participants can produce sounds by feet in collaboration with each other. Thedeveloped instrument, beacon, is regarded as embodied soundmedia product that will provide an interactive environmentaround it. The beacon produces laser beams lying on theground and rotating. Audio sounds are then produced whenthe beams pass individual performer's foot. As the performers are able to control the pitch and sound length accordingto the foot location and angles facing the instrument, theperformer's body motion and foot behavior can be translated into sound and music in an intuitive manner.}
}

@inproceedings{Sjuve2008,
  author = {Sjuve, Eva},
  title = {Prototype GO : Wireless Controller for Pure Data},
  pages = {362--363},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179629},
  url = {http://www.nime.org/proceedings/2008/nime2008_362.pdf},
  keywords = {Wireless controller, Pure Data, Gestural interface, Interactive Lights. },
  abstract = {This paper describes the development of a wireless wearablecontroller, GO, for both sound processing and interactionwith wearable lights. Pure Data is used for sound processing.The GO prototype is built using a PIC microcontroller usingvarious sensors for receiving information from physicalmovements.}
}

@inproceedings{Macrae2008,
  author = {Macrae, Robert and Dixon, Simon},
  title = {From Toy to Tutor : Note-Scroller is a Game to Teach Music},
  pages = {364--365},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179593},
  url = {http://www.nime.org/proceedings/2008/nime2008_364.pdf},
  keywords = {Graphical Interface, Computer Game, MIDI Display }
}

@inproceedings{Favilla2008,
  author = {Favilla, Stuart and Cannon, Joanne and Hicks, Tony and Chant, Dale and Favilla, Paris},
  title = {Gluisax : Bent Leather Band's Augmented Saxophone Project},
  pages = {366--369},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179531},
  url = {http://www.nime.org/proceedings/2008/nime2008_366.pdf},
  keywords = {Augmented saxophone, Gluion, OSC, virtuosic performance systems },
  abstract = {This demonstration presents three new augmented and metasaxophone interface/instruments, built by the Bent LeatherBand. The instruments are designed for virtuosic liveperformance and make use of Sukandar Kartadinata's Gluion[OSC] interfaces. The project rationale and research outcomesfor the first twelve months is discussed. Instruments/interfacesdescribed include the Gluisop, Gluialto and Leathersop.}
}

@inproceedings{DeJong2008,
  author = {de Jong, Staas},
  title = {The Cyclotactor : Towards a Tactile Platform for Musical Interaction},
  pages = {370--371},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179571},
  url = {http://www.nime.org/proceedings/2008/nime2008_370.pdf},
  keywords = {nime08}
}

@inproceedings{Demey2008,
  author = {Demey, Michiel and Leman, Marc and Bossuyt, Frederick and Vanfleteren, Jan},
  title = {The Musical Synchrotron : Using Wireless Motion Sensors to Study How Social Interaction Affects Synchronization with Musical Tempo},
  pages = {372--373},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2008},
  address = {Genoa, Italy},
  issn = {2220-4806},
  doi = {10.5281/zenodo.1179521},
  url = {http://www.nime.org/proceedings/2008/nime2008_372.pdf},
  keywords = {Wireless sensors, tempo perception, social interaction, music and movement, embodied music cognition },
  abstract = {The Musical Synchrotron is a software interface that connects wireless motion sensors to a real-time interactive environment (Pure Data, Max/MSP). In addition to the measurement of movement, the system provides audio playback and visual feedback. The Musical Synchrotron outputs a score with the degree in which synchronization with the presented music is successful. The interface has been used to measure how people move in response to music. The system was used for experiments at public events. }
}

