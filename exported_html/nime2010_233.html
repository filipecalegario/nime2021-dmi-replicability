
        <!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <meta name="description" content="">
            <meta name="author" content="Filipe Calegario, Mark Otto, Jacob Thornton, and Bootstrap contributors">
            <meta name="generator" content="Hugo 0.79.0">
            <title>nime2010_233 - Multimodal Musician Recognition</title>

            <link rel="canonical" href="https://getbootstrap.com/docs/5.0/examples/album/">

            <!-- Bootstrap core CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

        </head>
        <body>
            
        <header>
        <div class="navbar navbar-dark bg-dark shadow-sm">
            <div class="container">
            <a href="#" class="navbar-brand d-flex align-items-center">
                <strong>NIME 2021 - DMI Replicability - nime2010_233</strong>
            </a>
            </div>
        </div>
        </header>

        <main>

        <section class="py-3 text-left container">
            <div class="row py-lg-5">
            <div class="col-lg-12 col-md-8 mx-auto">
                <h2 class="fw-light">Multimodal Musician Recognition</h2>
                <h5 class="fw-light text-muted">Hochenbaum, Jordan and Kapur, Ajay and Wright, Matthew</h5>
                <h6 class="fw-light text-muted">Length: 5 pages</h6>
                <h4 class="py-2 fw-light">Abstract</h4>
                <p class="text-muted">This research is an initial effort in showing how a multimodal approach can improve systems for gaining insight into a musician's practice and technique. Embedding a variety of sensors inside musical instruments and synchronously recording the sensors' data along with audio, we gather a database of gestural information from multiple performers, then use machine-learning techniques to recognize which musician is performing. Our multimodal approach (using both audio and sensor data) yields promising performer classification results, which we see as a first step in a larger effort to gain insight into musicians' practice and technique. </p>
                <h4 class="fw-light">Extracted URLs from PDF</h4>
                
                <div class="alert alert-danger" role="alert">
                    No URL found in the paper.
                </div>
        
           
                
                <p>
                <p>
                <a href="http://www.nime.org/proceedings/2010/nime2010_233.pdf" target="_blank" class="btn btn-primary my-2">Original PDF</a>             
                </p>
            </div>
            <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSeeNTTMW8wbwNWr8AYITCU_78yFpR5Uyn0YRZG9iOcisxUCzw/viewform?embedded=true&usp=pp_url&entry.702570504=nime2010_233&entry.1715562253=Multimodal+Musician+Recognition" width="700" height="600" frameborder="0" marginheight="0" marginwidth="0">Loadingâ€¦</iframe>
            </div>

         </section>

        </main>

        <footer class="text-muted py-5">
        <div class="container">
            <p class="float-end mb-1">
            <a href="#">Back to top</a>
            </p>
        </div>
        </footer>
        
        </body>
        </html>
    