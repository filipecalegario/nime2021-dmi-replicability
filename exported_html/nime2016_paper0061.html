
        <!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <meta name="description" content="">
            <meta name="author" content="Filipe Calegario, Mark Otto, Jacob Thornton, and Bootstrap contributors">
            <meta name="generator" content="Hugo 0.79.0">
            <title>nime2016_paper0061 - x2Gesture: how machines could learn expressive gesture variations of expert musicians</title>

            <link rel="canonical" href="https://getbootstrap.com/docs/5.0/examples/album/">

            <!-- Bootstrap core CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

        </head>
        <body>
            
        <header>
        <div class="navbar navbar-dark bg-dark shadow-sm">
            <div class="container">
            <a href="#" class="navbar-brand d-flex align-items-center">
                <strong>NIME 2021 - DMI Replicability - nime2016_paper0061</strong>
            </a>
            </div>
        </div>
        </header>

        <main>

        <section class="py-3 text-left container">
            <div class="row py-lg-5">
            <div class="col-lg-12 col-md-8 mx-auto">
                <h2 class="fw-light">x2Gesture: how machines could learn expressive gesture variations of expert musicians</h2>
                <h5 class="fw-light text-muted">Christina Volioti and Sotiris Manitsaris and Eleni Katsouli and Athanasios Manitsaris</h5>
                <h6 class="fw-light text-muted">Length: 6 pages</h6>
                <h4 class="py-2 fw-light">Abstract</h4>
                <p class="text-muted">There is a growing interest in `unlocking' the motor
skills of expert musicians. Motivated by this need, the main objective of this
paper is to present a new way of modeling expressive gesture variations in
musical performance. For this purpose, the 3D gesture recognition engine
`x2Gesture' (eXpert eXpressive Gesture) has been developed, inspired
by the Gesture Variation Follower, which is initially designed and developed at
IRCAM in Paris and then extended at Goldsmiths College in London. x2Gesture
supports both learning of musical gestures and live performing, through gesture
sonification, as a unified user experience. The deeper understanding of the
expressive gestural variations permits to define the confidence bounds of the
expert's gestures, which are used during the decoding phase of the
recognition. The first experiments show promising results in terms of recognition
accuracy and temporal alignment between template and performed gesture, which
leads to a better fluidity and immediacy and thus gesture sonification. </p>
                <h4 class="fw-light">Extracted URLs from PDF</h4>
                <ul>
                    <li><a href="https://github.com/bcaramiaux/ofxGVF" target="_blank">https://github.com/bcaramiaux/ofxGVF</a></li>
                    <li><a href="http://synertial.com/" target="_blank">http://synertial.com/</a></li>
                    <li><a href="http://www.indiana.edu/~deanfac/blfal03/mus/mus_t561_" target="_blank">http://www.indiana.edu/~deanfac/blfal03/mus/mus_t561_</a></li>
                </ul>
           
                
        <div class="alert alert-warning" role="alert">
                <a class="alert-link" data-toggle="collapse" href="#debug" aria-expanded="false" aria-controls="debug">
                    Debug: if a extracted URL is malformed
                </a>
                        <ul class="font-monospace collapse" id="debug">

                            <li><small>https://github.com/bcaramiaux/ofxGVF 50 100 150 200 250 300 350 -1.0 -0.5 0.0 0.5 1.0 frames data for x axis Higher bound</small></li>
                    <li><small>http://synertial.com/ 5.1 Evaluation of case study I For the evaluation of the learning scenario, jackknife method was </small></li>
                    <li><small>http://www.indiana.edu/~deanfac/blfal03/mus/mus_t561_9824.html (Accessed 5 January 2016).  [20] P. Kolesnik, and M.M. Wanderley. Implementation of the </small></li>

                        </ul>
        </div>
        
                <p>
                <p>
                <a href="http://www.nime.org/proceedings/2016/nime2016_paper0061.pdf" target="_blank" class="btn btn-primary my-2">Original PDF</a>             
                </p>
            </div>
            <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSeeNTTMW8wbwNWr8AYITCU_78yFpR5Uyn0YRZG9iOcisxUCzw/viewform?embedded=true&usp=pp_url&entry.702570504=nime2016_paper0061&entry.1715562253=x2Gesture:+how+machines+could+learn+expressive+gesture+variations+of+expert+musicians" width="700" height="600" frameborder="0" marginheight="0" marginwidth="0">Loadingâ€¦</iframe>
            </div>

         </section>

        </main>

        <footer class="text-muted py-5">
        <div class="container">
            <p class="float-end mb-1">
            <a href="#">Back to top</a>
            </p>
        </div>
        </footer>
        
        </body>
        </html>
    