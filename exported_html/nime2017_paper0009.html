
        <!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <meta name="description" content="">
            <meta name="author" content="Filipe Calegario, Mark Otto, Jacob Thornton, and Bootstrap contributors">
            <meta name="generator" content="Hugo 0.79.0">
            <title>nime2017_paper0009 - Towards Robust Tracking with an Unreliable Motion Sensor Using Machine Learning</title>

            <link rel="canonical" href="https://getbootstrap.com/docs/5.0/examples/album/">

            <!-- Bootstrap core CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

        </head>
        <body>
            
        <header>
        <div class="navbar navbar-dark bg-dark shadow-sm">
            <div class="container">
            <a href="#" class="navbar-brand d-flex align-items-center">
                <strong>NIME 2021 - DMI Replicability - nime2017_paper0009</strong>
            </a>
            </div>
        </div>
        </header>

        <main>

        <section class="py-3 text-left container">
            <div class="row py-lg-5">
            <div class="col-lg-12 col-md-8 mx-auto">
                <h2 class="fw-light">Towards Robust Tracking with an Unreliable Motion Sensor Using Machine Learning</h2>
                <h5 class="fw-light text-muted">Jiayue Wu and Mark Rau and Yun Zhang and Yijun Zhou and Matt Wright</h5>
                <h6 class="fw-light text-muted">Length: 6 pages</h6>
                <h4 class="py-2 fw-light">Abstract</h4>
                <p class="text-muted">This paper presents solutions to improve reliability and to work around challenges of using a Leap Motion; sensor as a gestural control and input device in digital music instrument (DMI) design. We implement supervised learning algorithms (k-nearest neighbors, support vector machine, binary decision tree, and artificial neural network) to estimate hand motion data, which is not typically captured by the sensor. Two problems are addressed: 1) the sensor cannot detect overlapping hands 2) The sensor's limited detection range. Training examples included 7 kinds of overlapping hand gestures as well as hand trajectories where a hand goes out of the sensor's range. The overlapping gestures were treated as a classification problem and the best performing model was k-nearest neighbors with 62% accuracy. The out-of-range problem was treated first as a clustering problem to group the training examples into a small number of trajectory types, then as a classification problem to predict trajectory type based on the hand's motion before going out of range. The best performing model was k-nearest neighbors with an accuracy of 30%. The prediction models were implemented in an ongoing multimedia electroacoustic vocal performance and an educational project named Embodied Sonic Meditation (ESM).  </p>
                <h4 class="fw-light">Extracted URLs from PDF</h4>
                <ul>
                    <li><a href="https://www.ccs.ucsb.edu/" target="_blank">https://www.ccs.ucsb.edu/</a></li>
                    <li><a href="https://developer.leapmotion.com/" target="_blank">https://developer.leapmotion.com/</a></li>
                    <li><a href="https://pypi.python.org/pypi/ntplib/" target="_blank">https://pypi.python.org/pypi/ntplib/</a></li>
                    <li><a href="https://www.mathworks.com/products/statistics/" target="_blank">https://www.mathworks.com/products/statistics/</a></li>
                    <li><a href="http://scikit-learn.org/stable/" target="_blank">http://scikit-learn.org/stable/</a></li>
                    <li><a href="http://leenissen.dk/fann/wp/" target="_blank">http://leenissen.dk/fann/wp/</a></li>
                    <li><a href="http://blog.leapmotion.com/mobile-platform/" target="_blank">http://blog.leapmotion.com/mobile-platform/</a></li>
                </ul>
           
                
        <div class="alert alert-warning" role="alert">
                <a class="alert-link" data-toggle="collapse" href="#debug" aria-expanded="false" aria-controls="debug">
                    Debug: if a extracted URL is malformed
                </a>
                        <ul class="font-monospace collapse" id="debug">

                            <li><small>https://www.ccs.ucsb.edu/ 2 https://developer.leapmotion.com/ 42  Figure 2. The coordinates of Leap Motion interactionBo</small></li>
                    <li><small>https://developer.leapmotion.com/ 42  Figure 2. The coordinates of Leap Motion interactionBox  However, the sensor’s small rang</small></li>
                    <li><small>https://pypi.python.org/pypi/ntplib/  Our dataset contains 292 examples extracted from recorded training data; we split them randomly </small></li>
                    <li><small>https://www.mathworks.com/products/statistics/ 5 http://scikit-learn.org/stable/                                   where αi  are Lagrange mult</small></li>
                    <li><small>http://scikit-learn.org/stable/                                   where αi  are Lagrange multipliers of a dual optimization prob</small></li>
                    <li><small>http://leenissen.dk/fann/wp/ Table 1. Test accuracy of SVM, BDT and KNN models 7.2 Overlapping Gesture Classification We impl</small></li>
                    <li><small>http://blog.leapmotion.com/mobile-platform/ [2] Bongers, Bert. "An interview with Sensorband." Computer Music Journal 22, no. 1 (1998): 13-24</small></li>

                        </ul>
        </div>
        
                <p>
                <p>
                <a href="http://www.nime.org/proceedings/2017/nime2017_paper0009.pdf" target="_blank" class="btn btn-primary my-2">Original PDF</a>             
                </p>
            </div>
            <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSeeNTTMW8wbwNWr8AYITCU_78yFpR5Uyn0YRZG9iOcisxUCzw/viewform?embedded=true&usp=pp_url&entry.702570504=nime2017_paper0009&entry.1715562253=Towards+Robust+Tracking+with+an+Unreliable+Motion+Sensor+Using+Machine+Learning" width="700" height="600" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
            </div>

         </section>

        </main>

        <footer class="text-muted py-5">
        <div class="container">
            <p class="float-end mb-1">
            <a href="#">Back to top</a>
            </p>
        </div>
        </footer>
        
        </body>
        </html>
    