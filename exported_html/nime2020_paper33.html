
        <!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <meta name="description" content="">
            <meta name="author" content="Filipe Calegario, Mark Otto, Jacob Thornton, and Bootstrap contributors">
            <meta name="generator" content="Hugo 0.79.0">
            <title>Review Page</title>

            <link rel="canonical" href="https://getbootstrap.com/docs/5.0/examples/album/">

            <!-- Bootstrap core CSS -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

        </head>
        <body>
            
        <header>
        <div class="navbar navbar-dark bg-dark shadow-sm">
            <div class="container">
            <a href="#" class="navbar-brand d-flex align-items-center">
                <strong>NIME 2021 - DMI Replicability - nime2020_paper33</strong>
            </a>
            </div>
        </div>
        </header>

        <main>

        <section class="py-3 text-left container">
            <div class="row py-lg-5">
            <div class="col-lg-12 col-md-8 mx-auto">
                <h2 class="fw-light">P(l)aying Attention: Multi-modal, multi-temporal music control</h2>
                <h5 class="fw-light text-muted">Gold, Nicolas E and Wang, Chongyang and Olugbade, Temitayo and Berthouze, Nadia and Williams, Amanda</h5>
                <h6 class="fw-light text-muted">Length: 4 pages</h6>
                <h4 class="py-2 fw-light">Abstract</h4>
                <p class="text-muted">The expressive control of sound and music through body movements is well-studied.  For some people, body movement is demanding, and although they would prefer to express themselves freely using gestural control, they are unable to use such interfaces without difficulty.  In this paper, we present the P(l)aying Attention framework for manipulating recorded music to support these people, and to help the therapists that work with them. The aim is to facilitate body awareness, exploration, and expressivity by allowing the manipulation of a pre-recorded ‘ensemble’ through an interpretation of body movement, provided by a machine-learning system trained on physiotherapist assessments and movement data from people with chronic pain.  The system considers the nature of a person’s movement (e.g. protective) and offers an interpretation in terms of the joint-groups that are playing a major role in the determination at that point in the movement, and to which attention should perhaps be given (or the opposite at the user’s discretion).  Using music to convey the interpretation offers informational (through movement sonification) and creative (through manipulating the ensemble by movement) possibilities.  The approach offers the opportunity to explore movement and music at multiple timescales and under varying musical aesthetics.</p>
                <h4 class="fw-light">Extracted URLs from PDF</h4>
                <ul>
                    <li><a href="https://processing.org" target="_blank">https://processing.org</a></li>
                    <li><a href="https://www.ableton.com/en/live/" target="_blank">https://www.ableton.com/en/live/</a></li>
                    <li><a href="http://resolver.staatsbibliothek-berlin.de/" target="_blank">http://resolver.staatsbibliothek-berlin.de/</a></li>
                </ul>
                <div class="alert alert-warning" role="alert">
                <a class="alert-link" data-toggle="collapse" href="#debug" aria-expanded="false" aria-controls="debug">
                    Debug: if a extracted URL is malformed
                </a>
                <ul class="font-monospace collapse" id="debug">
                    <li><small>https://processing.org, lastaccessed 29 Jan 2020.[2] Ableton AG. Live, n.d.https://www.ableton.com/en/live/, last acces</small></li>
                    <li><small>https://www.ableton.com/en/live/, last accessed 29Jan 2020.[3] A.-P. Andersson and B. Cappelen. Musical interactionfor health imp</small></li>
                    <li><small>http://resolver.staatsbibliothek-berlin.de/SBB0000A73000000000.[15] N. Schaffert, T. B. Janzen, K. Mattes, and M. H.Thaut. A review on the r</small></li>
                </ul>
                </div>
                <p>
                <p>
                <a href="https://www.nime.org/proceedings/2020/nime2020_paper33.pdf" target="_blank" class="btn btn-primary my-2">Original PDF</a>             
                </p>
            </div>
            <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSeeNTTMW8wbwNWr8AYITCU_78yFpR5Uyn0YRZG9iOcisxUCzw/viewform?usp=pp_url&entry.702570504=nime2020_paper33&entry.1715562253=P(l)aying+Attention:+Multi-modal,+multi-temporal+music+control" width="700" height="600" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
            </div>

            
            
         </section>
        

        </main>

        <footer class="text-muted py-5">
        <div class="container">
            <p class="float-end mb-1">
            <a href="#">Back to top</a>
            </p>
        </div>
        </footer>
        
        </body>
        </html>
    